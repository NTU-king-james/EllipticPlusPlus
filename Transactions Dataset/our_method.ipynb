{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.graph_objs as go \n",
    "import plotly.offline as py \n",
    "import math\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=19q09IFhfkOOBOXvn_dKhWjILJtjCcsjc\n",
      "From (redirected): https://drive.google.com/uc?id=19q09IFhfkOOBOXvn_dKhWjILJtjCcsjc&confirm=t&uuid=253c15ff-8802-4f69-be84-17b4f1fce4af\n",
      "To: /Users/fffuuuming/Desktop/study/master_1_2/fintech/final_project/impl/elliptic-fork/Transactions Dataset/txs_features.csv\n",
      "100%|████████████████████████████████████████| 695M/695M [00:44<00:00, 15.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1DiBxn8TXdbJqoSw58pYUeaqO3oOKhuQO\n",
      "To: /Users/fffuuuming/Desktop/study/master_1_2/fintech/final_project/impl/elliptic-fork/Transactions Dataset/txs_classes.csv\n",
      "100%|██████████████████████████████████████| 2.36M/2.36M [00:00<00:00, 11.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Q2yG_CIDvfdGP-fKVPSw979EYgQukjz5\n",
      "To: /Users/fffuuuming/Desktop/study/master_1_2/fintech/final_project/impl/elliptic-fork/Transactions Dataset/txs_edgelist.csv\n",
      "100%|██████████████████████████████████████| 4.47M/4.47M [00:00<00:00, 16.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "!gdown --fuzzy \"https://drive.google.com/file/d/19q09IFhfkOOBOXvn_dKhWjILJtjCcsjc/view?usp=drive_link\"\n",
    "!gdown --fuzzy \"https://drive.google.com/file/d/1DiBxn8TXdbJqoSw58pYUeaqO3oOKhuQO/view?usp=drive_link\"\n",
    "!gdown --fuzzy \"https://drive.google.com/file/d/1Q2yG_CIDvfdGP-fKVPSw979EYgQukjz5/view?usp=drive_link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transaction features: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>in_BTC_min</th>\n",
       "      <th>in_BTC_max</th>\n",
       "      <th>in_BTC_mean</th>\n",
       "      <th>in_BTC_median</th>\n",
       "      <th>in_BTC_total</th>\n",
       "      <th>out_BTC_min</th>\n",
       "      <th>out_BTC_max</th>\n",
       "      <th>out_BTC_mean</th>\n",
       "      <th>out_BTC_median</th>\n",
       "      <th>out_BTC_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>1.668990e-01</td>\n",
       "      <td>0.367074</td>\n",
       "      <td>0.266986</td>\n",
       "      <td>0.266986</td>\n",
       "      <td>0.533972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11108</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.137586</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.127429</td>\n",
       "      <td>...</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.861940e-01</td>\n",
       "      <td>5.025584</td>\n",
       "      <td>2.805889</td>\n",
       "      <td>2.805889</td>\n",
       "      <td>5.611778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51816</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.170103</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>2.279902e-01</td>\n",
       "      <td>0.228518</td>\n",
       "      <td>0.228254</td>\n",
       "      <td>0.228254</td>\n",
       "      <td>0.456508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68869</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.114267</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>0.547008</td>\n",
       "      <td>-0.161652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.102967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.308900</td>\n",
       "      <td>1.229000e+00</td>\n",
       "      <td>8.079800</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>9.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89273</td>\n",
       "      <td>1</td>\n",
       "      <td>5.202107</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>260.090707</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>5.335864</td>\n",
       "      <td>...</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>1.300000e-07</td>\n",
       "      <td>41.264036</td>\n",
       "      <td>0.065016</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>852.164680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203764</th>\n",
       "      <td>158304003</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.165622</td>\n",
       "      <td>-0.139563</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.156113</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203765</th>\n",
       "      <td>158303998</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.167040</td>\n",
       "      <td>-0.139563</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.157564</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203766</th>\n",
       "      <td>158303966</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.167040</td>\n",
       "      <td>-0.139563</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.157564</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203767</th>\n",
       "      <td>161526077</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.172212</td>\n",
       "      <td>-0.139573</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203768</th>\n",
       "      <td>194103537</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.172212</td>\n",
       "      <td>-0.139573</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203769 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  Time step  Local_feature_1  Local_feature_2  \\\n",
       "0            3321          1        -0.169615        -0.184668   \n",
       "1           11108          1        -0.137586        -0.184668   \n",
       "2           51816          1        -0.170103        -0.184668   \n",
       "3           68869          1        -0.114267        -0.184668   \n",
       "4           89273          1         5.202107        -0.210553   \n",
       "...           ...        ...              ...              ...   \n",
       "203764  158304003         49        -0.165622        -0.139563   \n",
       "203765  158303998         49        -0.167040        -0.139563   \n",
       "203766  158303966         49        -0.167040        -0.139563   \n",
       "203767  161526077         49        -0.172212        -0.139573   \n",
       "203768  194103537         49        -0.172212        -0.139573   \n",
       "\n",
       "        Local_feature_3  Local_feature_4  Local_feature_5  Local_feature_6  \\\n",
       "0             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "1             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "2             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "3             -1.201369         0.028105        -0.043875        -0.113002   \n",
       "4             -1.756361        -0.121970       260.090707        -0.113002   \n",
       "...                 ...              ...              ...              ...   \n",
       "203764         1.018602        -0.121970        -0.043875        -0.113002   \n",
       "203765         1.018602        -0.121970        -0.043875        -0.113002   \n",
       "203766         1.018602        -0.121970        -0.043875        -0.113002   \n",
       "203767         1.018602        -0.121970        -0.043875        -0.113002   \n",
       "203768         1.018602        -0.121970        -0.043875        -0.113002   \n",
       "\n",
       "        Local_feature_7  Local_feature_8  ...  in_BTC_min  in_BTC_max  \\\n",
       "0             -0.061584        -0.160199  ...    0.534072    0.534072   \n",
       "1             -0.061584        -0.127429  ...    5.611878    5.611878   \n",
       "2             -0.061584        -0.160699  ...    0.456608    0.456608   \n",
       "3              0.547008        -0.161652  ...    0.308900    8.000000   \n",
       "4             -0.061584         5.335864  ...  852.164680  852.164680   \n",
       "...                 ...              ...  ...         ...         ...   \n",
       "203764        -0.061584        -0.156113  ...         NaN         NaN   \n",
       "203765        -0.061584        -0.157564  ...         NaN         NaN   \n",
       "203766        -0.061584        -0.157564  ...         NaN         NaN   \n",
       "203767        -0.061584        -0.162856  ...         NaN         NaN   \n",
       "203768        -0.061584        -0.162856  ...         NaN         NaN   \n",
       "\n",
       "        in_BTC_mean  in_BTC_median  in_BTC_total   out_BTC_min  out_BTC_max  \\\n",
       "0          0.534072       0.534072      0.534072  1.668990e-01     0.367074   \n",
       "1          5.611878       5.611878      5.611878  5.861940e-01     5.025584   \n",
       "2          0.456608       0.456608      0.456608  2.279902e-01     0.228518   \n",
       "3          3.102967       1.000000      9.308900  1.229000e+00     8.079800   \n",
       "4        852.164680     852.164680    852.164680  1.300000e-07    41.264036   \n",
       "...             ...            ...           ...           ...          ...   \n",
       "203764          NaN            NaN           NaN           NaN          NaN   \n",
       "203765          NaN            NaN           NaN           NaN          NaN   \n",
       "203766          NaN            NaN           NaN           NaN          NaN   \n",
       "203767          NaN            NaN           NaN           NaN          NaN   \n",
       "203768          NaN            NaN           NaN           NaN          NaN   \n",
       "\n",
       "        out_BTC_mean  out_BTC_median  out_BTC_total  \n",
       "0           0.266986        0.266986       0.533972  \n",
       "1           2.805889        2.805889       5.611778  \n",
       "2           0.228254        0.228254       0.456508  \n",
       "3           4.654400        4.654400       9.308800  \n",
       "4           0.065016        0.000441     852.164680  \n",
       "...              ...             ...            ...  \n",
       "203764           NaN             NaN            NaN  \n",
       "203765           NaN             NaN            NaN  \n",
       "203766           NaN             NaN            NaN  \n",
       "203767           NaN             NaN            NaN  \n",
       "203768           NaN             NaN            NaN  \n",
       "\n",
       "[203769 rows x 184 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transaction classes: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11108</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51816</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68869</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89273</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203764</th>\n",
       "      <td>158304003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203765</th>\n",
       "      <td>158303998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203766</th>\n",
       "      <td>158303966</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203767</th>\n",
       "      <td>161526077</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203768</th>\n",
       "      <td>194103537</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203769 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  class\n",
       "0            3321      3\n",
       "1           11108      3\n",
       "2           51816      3\n",
       "3           68869      2\n",
       "4           89273      2\n",
       "...           ...    ...\n",
       "203764  158304003      3\n",
       "203765  158303998      3\n",
       "203766  158303966      3\n",
       "203767  161526077      3\n",
       "203768  194103537      3\n",
       "\n",
       "[203769 rows x 2 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transaction-Transaction edgelist: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId1</th>\n",
       "      <th>txId2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>5530458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>232438397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230460314</td>\n",
       "      <td>230459870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230333930</td>\n",
       "      <td>230595899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232013274</td>\n",
       "      <td>232029206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234350</th>\n",
       "      <td>158365409</td>\n",
       "      <td>157930723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234351</th>\n",
       "      <td>188708874</td>\n",
       "      <td>188708879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234352</th>\n",
       "      <td>157659064</td>\n",
       "      <td>157659046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234353</th>\n",
       "      <td>87414554</td>\n",
       "      <td>106877725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234354</th>\n",
       "      <td>158589452</td>\n",
       "      <td>158589457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234355 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            txId1      txId2\n",
       "0       230425980    5530458\n",
       "1       232022460  232438397\n",
       "2       230460314  230459870\n",
       "3       230333930  230595899\n",
       "4       232013274  232029206\n",
       "...           ...        ...\n",
       "234350  158365409  157930723\n",
       "234351  188708874  188708879\n",
       "234352  157659064  157659046\n",
       "234353   87414554  106877725\n",
       "234354  158589452  158589457\n",
       "\n",
       "[234355 rows x 2 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTransaction features: \\n\")\n",
    "df_txs_features = pd.read_csv(\"txs_features.csv\")\n",
    "df_txs_features\n",
    "\n",
    "print(\"\\nTransaction classes: \\n\")\n",
    "df_txs_classes = pd.read_csv(\"txs_classes.csv\")\n",
    "df_txs_classes\n",
    "\n",
    "print(\"\\nTransaction-Transaction edgelist: \\n\")\n",
    "df_txs_edgelist = pd.read_csv(\"txs_edgelist.csv\")\n",
    "df_txs_edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop nodes (transactions) without augmented feature values (0.5% not de-anonymized)\n",
    "- Drop edges accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 把 feature DataFrame 裡含 NaN 的行丟掉\n",
    "df_txs_features = df_txs_features.dropna()\n",
    "# 2. 取出還在的 txId（假設 index 就是 txId）\n",
    "valid_tx = set(df_txs_features.txId)\n",
    "\n",
    "# 3. 篩邊表：只保留 txId1 與 txId2 都在 valid_tx 裡面的邊\n",
    "df_txs_edgelist = df_txs_edgelist[\n",
    "    df_txs_edgelist['txId1'].isin(valid_tx) &\n",
    "    df_txs_edgelist['txId2'].isin(valid_tx)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId1</th>\n",
       "      <th>txId2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>5530458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>232022460</td>\n",
       "      <td>232438397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230460314</td>\n",
       "      <td>230459870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230333930</td>\n",
       "      <td>230595899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232013274</td>\n",
       "      <td>232029206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233278</th>\n",
       "      <td>158365409</td>\n",
       "      <td>157930723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233279</th>\n",
       "      <td>188708874</td>\n",
       "      <td>188708879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233280</th>\n",
       "      <td>157659064</td>\n",
       "      <td>157659046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233281</th>\n",
       "      <td>87414554</td>\n",
       "      <td>106877725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233282</th>\n",
       "      <td>158589452</td>\n",
       "      <td>158589457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233283 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            txId1      txId2\n",
       "0       230425980    5530458\n",
       "1       232022460  232438397\n",
       "2       230460314  230459870\n",
       "3       230333930  230595899\n",
       "4       232013274  232029206\n",
       "...           ...        ...\n",
       "233278  158365409  157930723\n",
       "233279  188708874  188708879\n",
       "233280  157659064  157659046\n",
       "233281   87414554  106877725\n",
       "233282  158589452  158589457\n",
       "\n",
       "[233283 rows x 2 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txs_edgelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge class into txs_features for later index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_txs_classes[['txId', 'class']]\n",
    "\n",
    "# 2. 用 merge 把 class 欄併到 txs_features\n",
    "df_txs_features = df_txs_features.merge(\n",
    "    subset,\n",
    "    on='txId',     # 以 txId 當作 key\n",
    "    how='left'     # 保留 txs_features 所有列，對不到的 class 欄會是 NaN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>in_BTC_max</th>\n",
       "      <th>in_BTC_mean</th>\n",
       "      <th>in_BTC_median</th>\n",
       "      <th>in_BTC_total</th>\n",
       "      <th>out_BTC_min</th>\n",
       "      <th>out_BTC_max</th>\n",
       "      <th>out_BTC_mean</th>\n",
       "      <th>out_BTC_median</th>\n",
       "      <th>out_BTC_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>0.534072</td>\n",
       "      <td>1.668990e-01</td>\n",
       "      <td>0.367074</td>\n",
       "      <td>0.266986</td>\n",
       "      <td>0.266986</td>\n",
       "      <td>0.533972</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11108</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.137586</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.127429</td>\n",
       "      <td>...</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.611878</td>\n",
       "      <td>5.861940e-01</td>\n",
       "      <td>5.025584</td>\n",
       "      <td>2.805889</td>\n",
       "      <td>2.805889</td>\n",
       "      <td>5.611778</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51816</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.170103</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>0.456608</td>\n",
       "      <td>2.279902e-01</td>\n",
       "      <td>0.228518</td>\n",
       "      <td>0.228254</td>\n",
       "      <td>0.228254</td>\n",
       "      <td>0.456508</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68869</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.114267</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>0.547008</td>\n",
       "      <td>-0.161652</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.102967</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.308900</td>\n",
       "      <td>1.229000e+00</td>\n",
       "      <td>8.079800</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>4.654400</td>\n",
       "      <td>9.308800</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89273</td>\n",
       "      <td>1</td>\n",
       "      <td>5.202107</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>260.090707</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>5.335864</td>\n",
       "      <td>...</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>1.300000e-07</td>\n",
       "      <td>41.264036</td>\n",
       "      <td>0.065016</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>852.164680</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202799</th>\n",
       "      <td>194747812</td>\n",
       "      <td>49</td>\n",
       "      <td>0.558398</td>\n",
       "      <td>-0.198956</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.584665</td>\n",
       "      <td>...</td>\n",
       "      <td>115.952889</td>\n",
       "      <td>115.952889</td>\n",
       "      <td>115.952889</td>\n",
       "      <td>115.952889</td>\n",
       "      <td>1.653300e+00</td>\n",
       "      <td>114.299544</td>\n",
       "      <td>57.976422</td>\n",
       "      <td>57.976422</td>\n",
       "      <td>115.952844</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202800</th>\n",
       "      <td>194747925</td>\n",
       "      <td>49</td>\n",
       "      <td>0.547658</td>\n",
       "      <td>-0.198956</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.573676</td>\n",
       "      <td>...</td>\n",
       "      <td>114.250098</td>\n",
       "      <td>114.250098</td>\n",
       "      <td>114.250098</td>\n",
       "      <td>114.250098</td>\n",
       "      <td>2.035300e-02</td>\n",
       "      <td>114.229700</td>\n",
       "      <td>57.125027</td>\n",
       "      <td>57.125027</td>\n",
       "      <td>114.250053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202801</th>\n",
       "      <td>194748063</td>\n",
       "      <td>49</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>-0.198853</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.569524</td>\n",
       "      <td>...</td>\n",
       "      <td>113.606771</td>\n",
       "      <td>113.606771</td>\n",
       "      <td>113.606771</td>\n",
       "      <td>113.606771</td>\n",
       "      <td>9.257490e-01</td>\n",
       "      <td>112.680977</td>\n",
       "      <td>56.803363</td>\n",
       "      <td>56.803363</td>\n",
       "      <td>113.606726</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202802</th>\n",
       "      <td>194748070</td>\n",
       "      <td>49</td>\n",
       "      <td>0.537760</td>\n",
       "      <td>-0.198853</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.563549</td>\n",
       "      <td>...</td>\n",
       "      <td>112.680977</td>\n",
       "      <td>112.680977</td>\n",
       "      <td>112.680977</td>\n",
       "      <td>112.680977</td>\n",
       "      <td>3.026970e-01</td>\n",
       "      <td>112.378235</td>\n",
       "      <td>56.340466</td>\n",
       "      <td>56.340466</td>\n",
       "      <td>112.680932</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202803</th>\n",
       "      <td>194835939</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.170463</td>\n",
       "      <td>-0.152788</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.161066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.399769</td>\n",
       "      <td>0.399769</td>\n",
       "      <td>0.399769</td>\n",
       "      <td>0.399769</td>\n",
       "      <td>3.995460e-01</td>\n",
       "      <td>0.399546</td>\n",
       "      <td>0.399546</td>\n",
       "      <td>0.399546</td>\n",
       "      <td>0.399546</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202804 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  Time step  Local_feature_1  Local_feature_2  \\\n",
       "0            3321          1        -0.169615        -0.184668   \n",
       "1           11108          1        -0.137586        -0.184668   \n",
       "2           51816          1        -0.170103        -0.184668   \n",
       "3           68869          1        -0.114267        -0.184668   \n",
       "4           89273          1         5.202107        -0.210553   \n",
       "...           ...        ...              ...              ...   \n",
       "202799  194747812         49         0.558398        -0.198956   \n",
       "202800  194747925         49         0.547658        -0.198956   \n",
       "202801  194748063         49         0.543600        -0.198853   \n",
       "202802  194748070         49         0.537760        -0.198853   \n",
       "202803  194835939         49        -0.170463        -0.152788   \n",
       "\n",
       "        Local_feature_3  Local_feature_4  Local_feature_5  Local_feature_6  \\\n",
       "0             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "1             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "2             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "3             -1.201369         0.028105        -0.043875        -0.113002   \n",
       "4             -1.756361        -0.121970       260.090707        -0.113002   \n",
       "...                 ...              ...              ...              ...   \n",
       "202799        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202800        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202801        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202802        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202803         1.018602        -0.121970        -0.063725        -0.113002   \n",
       "\n",
       "        Local_feature_7  Local_feature_8  ...  in_BTC_max  in_BTC_mean  \\\n",
       "0             -0.061584        -0.160199  ...    0.534072     0.534072   \n",
       "1             -0.061584        -0.127429  ...    5.611878     5.611878   \n",
       "2             -0.061584        -0.160699  ...    0.456608     0.456608   \n",
       "3              0.547008        -0.161652  ...    8.000000     3.102967   \n",
       "4             -0.061584         5.335864  ...  852.164680   852.164680   \n",
       "...                 ...              ...  ...         ...          ...   \n",
       "202799        -0.061584         0.584665  ...  115.952889   115.952889   \n",
       "202800        -0.061584         0.573676  ...  114.250098   114.250098   \n",
       "202801        -0.061584         0.569524  ...  113.606771   113.606771   \n",
       "202802        -0.061584         0.563549  ...  112.680977   112.680977   \n",
       "202803        -0.061584        -0.161066  ...    0.399769     0.399769   \n",
       "\n",
       "        in_BTC_median  in_BTC_total   out_BTC_min  out_BTC_max  out_BTC_mean  \\\n",
       "0            0.534072      0.534072  1.668990e-01     0.367074      0.266986   \n",
       "1            5.611878      5.611878  5.861940e-01     5.025584      2.805889   \n",
       "2            0.456608      0.456608  2.279902e-01     0.228518      0.228254   \n",
       "3            1.000000      9.308900  1.229000e+00     8.079800      4.654400   \n",
       "4          852.164680    852.164680  1.300000e-07    41.264036      0.065016   \n",
       "...               ...           ...           ...          ...           ...   \n",
       "202799     115.952889    115.952889  1.653300e+00   114.299544     57.976422   \n",
       "202800     114.250098    114.250098  2.035300e-02   114.229700     57.125027   \n",
       "202801     113.606771    113.606771  9.257490e-01   112.680977     56.803363   \n",
       "202802     112.680977    112.680977  3.026970e-01   112.378235     56.340466   \n",
       "202803       0.399769      0.399769  3.995460e-01     0.399546      0.399546   \n",
       "\n",
       "        out_BTC_median  out_BTC_total  class  \n",
       "0             0.266986       0.533972      3  \n",
       "1             2.805889       5.611778      3  \n",
       "2             0.228254       0.456508      3  \n",
       "3             4.654400       9.308800      2  \n",
       "4             0.000441     852.164680      2  \n",
       "...                ...            ...    ...  \n",
       "202799       57.976422     115.952844      3  \n",
       "202800       57.125027     114.250053      3  \n",
       "202801       56.803363     113.606726      3  \n",
       "202802       56.340466     112.680932      3  \n",
       "202803        0.399546       0.399546      3  \n",
       "\n",
       "[202804 rows x 185 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txs_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize augmented features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notmalize augmented features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for column in df_txs_features.columns[-18:-1]:\n",
    "    scaler = MinMaxScaler()\n",
    "    df_txs_features[column] = scaler.fit_transform(df_txs_features[[column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txId</th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>in_BTC_max</th>\n",
       "      <th>in_BTC_mean</th>\n",
       "      <th>in_BTC_median</th>\n",
       "      <th>in_BTC_total</th>\n",
       "      <th>out_BTC_min</th>\n",
       "      <th>out_BTC_max</th>\n",
       "      <th>out_BTC_mean</th>\n",
       "      <th>out_BTC_median</th>\n",
       "      <th>out_BTC_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>8.301504e-05</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>8.904096e-05</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11108</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.137586</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.127429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>2.915711e-04</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>9.357923e-04</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51816</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.170103</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.134016e-04</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>7.612341e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68869</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.114267</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>0.547008</td>\n",
       "      <td>-0.161652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>6.113009e-04</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>1.552291e-03</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89273</td>\n",
       "      <td>1</td>\n",
       "      <td>5.202107</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>260.090707</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>5.335864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>6.466160e-11</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1.451405e-07</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202799</th>\n",
       "      <td>194747812</td>\n",
       "      <td>49</td>\n",
       "      <td>0.558398</td>\n",
       "      <td>-0.198956</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.584665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>8.223464e-04</td>\n",
       "      <td>0.010104</td>\n",
       "      <td>0.019336</td>\n",
       "      <td>1.933576e-02</td>\n",
       "      <td>0.010179</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202800</th>\n",
       "      <td>194747925</td>\n",
       "      <td>49</td>\n",
       "      <td>0.547658</td>\n",
       "      <td>-0.198956</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.573676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>1.012352e-05</td>\n",
       "      <td>0.010098</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>1.905181e-02</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202801</th>\n",
       "      <td>194748063</td>\n",
       "      <td>49</td>\n",
       "      <td>0.543600</td>\n",
       "      <td>-0.198853</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.569524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>4.604647e-04</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>1.894453e-02</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202802</th>\n",
       "      <td>194748070</td>\n",
       "      <td>49</td>\n",
       "      <td>0.537760</td>\n",
       "      <td>-0.198853</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>0.563549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>1.505606e-04</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.018790</td>\n",
       "      <td>1.879015e-02</td>\n",
       "      <td>0.009891</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202803</th>\n",
       "      <td>194835939</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.170463</td>\n",
       "      <td>-0.152788</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.161066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>1.987330e-04</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>1.332511e-04</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202804 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             txId  Time step  Local_feature_1  Local_feature_2  \\\n",
       "0            3321          1        -0.169615        -0.184668   \n",
       "1           11108          1        -0.137586        -0.184668   \n",
       "2           51816          1        -0.170103        -0.184668   \n",
       "3           68869          1        -0.114267        -0.184668   \n",
       "4           89273          1         5.202107        -0.210553   \n",
       "...           ...        ...              ...              ...   \n",
       "202799  194747812         49         0.558398        -0.198956   \n",
       "202800  194747925         49         0.547658        -0.198956   \n",
       "202801  194748063         49         0.543600        -0.198853   \n",
       "202802  194748070         49         0.537760        -0.198853   \n",
       "202803  194835939         49        -0.170463        -0.152788   \n",
       "\n",
       "        Local_feature_3  Local_feature_4  Local_feature_5  Local_feature_6  \\\n",
       "0             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "1             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "2             -1.201369        -0.121970        -0.043875        -0.113002   \n",
       "3             -1.201369         0.028105        -0.043875        -0.113002   \n",
       "4             -1.756361        -0.121970       260.090707        -0.113002   \n",
       "...                 ...              ...              ...              ...   \n",
       "202799        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202800        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202801        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202802        -0.091383        -0.121970        -0.043875        -0.113002   \n",
       "202803         1.018602        -0.121970        -0.063725        -0.113002   \n",
       "\n",
       "        Local_feature_7  Local_feature_8  ...  in_BTC_max  in_BTC_mean  \\\n",
       "0             -0.061584        -0.160199  ...    0.000047     0.000047   \n",
       "1             -0.061584        -0.127429  ...    0.000493     0.000493   \n",
       "2             -0.061584        -0.160699  ...    0.000040     0.000040   \n",
       "3              0.547008        -0.161652  ...    0.000702     0.000272   \n",
       "4             -0.061584         5.335864  ...    0.074805     0.074805   \n",
       "...                 ...              ...  ...         ...          ...   \n",
       "202799        -0.061584         0.584665  ...    0.010179     0.010179   \n",
       "202800        -0.061584         0.573676  ...    0.010029     0.010029   \n",
       "202801        -0.061584         0.569524  ...    0.009973     0.009973   \n",
       "202802        -0.061584         0.563549  ...    0.009891     0.009891   \n",
       "202803        -0.061584        -0.161066  ...    0.000035     0.000035   \n",
       "\n",
       "        in_BTC_median  in_BTC_total   out_BTC_min  out_BTC_max  out_BTC_mean  \\\n",
       "0            0.000047      0.000047  8.301504e-05     0.000032      0.000089   \n",
       "1            0.000493      0.000493  2.915711e-04     0.000444      0.000936   \n",
       "2            0.000040      0.000040  1.134016e-04     0.000020      0.000076   \n",
       "3            0.000088      0.000817  6.113009e-04     0.000714      0.001552   \n",
       "4            0.074805      0.074805  6.466160e-11     0.003648      0.000022   \n",
       "...               ...           ...           ...          ...           ...   \n",
       "202799       0.010179      0.010179  8.223464e-04     0.010104      0.019336   \n",
       "202800       0.010029      0.010029  1.012352e-05     0.010098      0.019052   \n",
       "202801       0.009973      0.009973  4.604647e-04     0.009961      0.018945   \n",
       "202802       0.009891      0.009891  1.505606e-04     0.009935      0.018790   \n",
       "202803       0.000035      0.000035  1.987330e-04     0.000035      0.000133   \n",
       "\n",
       "        out_BTC_median  out_BTC_total  class  \n",
       "0         8.904096e-05       0.000047      3  \n",
       "1         9.357923e-04       0.000493      3  \n",
       "2         7.612341e-05       0.000040      3  \n",
       "3         1.552291e-03       0.000817      2  \n",
       "4         1.451405e-07       0.074805      2  \n",
       "...                ...            ...    ...  \n",
       "202799    1.933576e-02       0.010179      3  \n",
       "202800    1.905181e-02       0.010029      3  \n",
       "202801    1.894453e-02       0.009973      3  \n",
       "202802    1.879015e-02       0.009891      3  \n",
       "202803    1.332511e-04       0.000035      3  \n",
       "\n",
       "[202804 rows x 185 columns]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time step</th>\n",
       "      <th>Local_feature_1</th>\n",
       "      <th>Local_feature_2</th>\n",
       "      <th>Local_feature_3</th>\n",
       "      <th>Local_feature_4</th>\n",
       "      <th>Local_feature_5</th>\n",
       "      <th>Local_feature_6</th>\n",
       "      <th>Local_feature_7</th>\n",
       "      <th>Local_feature_8</th>\n",
       "      <th>Local_feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>in_BTC_max</th>\n",
       "      <th>in_BTC_mean</th>\n",
       "      <th>in_BTC_median</th>\n",
       "      <th>in_BTC_total</th>\n",
       "      <th>out_BTC_min</th>\n",
       "      <th>out_BTC_max</th>\n",
       "      <th>out_BTC_mean</th>\n",
       "      <th>out_BTC_median</th>\n",
       "      <th>out_BTC_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>48</td>\n",
       "      <td>-0.168500</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163591</td>\n",
       "      <td>-0.164980</td>\n",
       "      <td>...</td>\n",
       "      <td>6.180084e-05</td>\n",
       "      <td>3.127478e-05</td>\n",
       "      <td>3.127597e-05</td>\n",
       "      <td>6.254833e-05</td>\n",
       "      <td>6.676470e-06</td>\n",
       "      <td>6.164291e-05</td>\n",
       "      <td>1.185148e-04</td>\n",
       "      <td>1.185150e-04</td>\n",
       "      <td>6.238815e-05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.170834</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163572</td>\n",
       "      <td>-0.167757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.350094e-05</td>\n",
       "      <td>9.975293e-06</td>\n",
       "      <td>5.421152e-06</td>\n",
       "      <td>2.992583e-05</td>\n",
       "      <td>9.815134e-08</td>\n",
       "      <td>2.194331e-05</td>\n",
       "      <td>1.622788e-05</td>\n",
       "      <td>8.509850e-06</td>\n",
       "      <td>2.990209e-05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3181</th>\n",
       "      <td>34</td>\n",
       "      <td>1.305212</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>97.300650</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>1.348765</td>\n",
       "      <td>1.321754</td>\n",
       "      <td>...</td>\n",
       "      <td>2.057192e-02</td>\n",
       "      <td>2.057192e-02</td>\n",
       "      <td>2.057192e-02</td>\n",
       "      <td>2.057192e-02</td>\n",
       "      <td>5.070912e-07</td>\n",
       "      <td>2.430414e-04</td>\n",
       "      <td>1.592931e-05</td>\n",
       "      <td>5.025955e-06</td>\n",
       "      <td>2.057192e-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>-0.166062</td>\n",
       "      <td>...</td>\n",
       "      <td>4.688075e-05</td>\n",
       "      <td>4.688076e-05</td>\n",
       "      <td>4.688195e-05</td>\n",
       "      <td>4.687833e-05</td>\n",
       "      <td>8.301504e-05</td>\n",
       "      <td>3.244988e-05</td>\n",
       "      <td>8.904078e-05</td>\n",
       "      <td>8.904096e-05</td>\n",
       "      <td>4.687265e-05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>48</td>\n",
       "      <td>-0.086232</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>17.046997</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.074885</td>\n",
       "      <td>-0.081943</td>\n",
       "      <td>...</td>\n",
       "      <td>1.207336e-03</td>\n",
       "      <td>1.207336e-03</td>\n",
       "      <td>1.207337e-03</td>\n",
       "      <td>1.207333e-03</td>\n",
       "      <td>5.184866e-08</td>\n",
       "      <td>2.625465e-04</td>\n",
       "      <td>5.313067e-06</td>\n",
       "      <td>7.031674e-07</td>\n",
       "      <td>1.207300e-03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403203785</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.172978</td>\n",
       "      <td>-0.172527</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163640</td>\n",
       "      <td>-0.169455</td>\n",
       "      <td>...</td>\n",
       "      <td>7.368421e-08</td>\n",
       "      <td>7.369663e-08</td>\n",
       "      <td>7.488595e-08</td>\n",
       "      <td>7.126406e-08</td>\n",
       "      <td>5.282356e-08</td>\n",
       "      <td>5.251141e-08</td>\n",
       "      <td>1.157616e-07</td>\n",
       "      <td>1.159417e-07</td>\n",
       "      <td>6.146496e-08</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403234712</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>-0.169142</td>\n",
       "      <td>...</td>\n",
       "      <td>4.387898e-06</td>\n",
       "      <td>4.387910e-06</td>\n",
       "      <td>4.389099e-06</td>\n",
       "      <td>4.385477e-06</td>\n",
       "      <td>2.477037e-05</td>\n",
       "      <td>4.401941e-06</td>\n",
       "      <td>1.660683e-05</td>\n",
       "      <td>1.660701e-05</td>\n",
       "      <td>4.371017e-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403234715</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>-0.169142</td>\n",
       "      <td>...</td>\n",
       "      <td>4.387898e-06</td>\n",
       "      <td>4.387910e-06</td>\n",
       "      <td>4.389099e-06</td>\n",
       "      <td>4.385477e-06</td>\n",
       "      <td>2.477037e-05</td>\n",
       "      <td>4.401941e-06</td>\n",
       "      <td>1.660683e-05</td>\n",
       "      <td>1.660701e-05</td>\n",
       "      <td>4.371017e-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403235564</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>-0.169142</td>\n",
       "      <td>...</td>\n",
       "      <td>4.387898e-06</td>\n",
       "      <td>4.387910e-06</td>\n",
       "      <td>4.389099e-06</td>\n",
       "      <td>4.385477e-06</td>\n",
       "      <td>2.477037e-05</td>\n",
       "      <td>4.401941e-06</td>\n",
       "      <td>1.660683e-05</td>\n",
       "      <td>1.660701e-05</td>\n",
       "      <td>4.371017e-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403244581</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.143292</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.133266</td>\n",
       "      <td>-0.139507</td>\n",
       "      <td>...</td>\n",
       "      <td>4.132228e-04</td>\n",
       "      <td>4.132228e-04</td>\n",
       "      <td>4.132240e-04</td>\n",
       "      <td>4.132204e-04</td>\n",
       "      <td>5.242564e-05</td>\n",
       "      <td>4.068115e-04</td>\n",
       "      <td>7.849469e-04</td>\n",
       "      <td>7.849471e-04</td>\n",
       "      <td>4.132059e-04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202804 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Time step  Local_feature_1  Local_feature_2  Local_feature_3  \\\n",
       "txId                                                                      \n",
       "1076              48        -0.168500         0.270909        -0.091383   \n",
       "2534               6        -0.170834        -0.131425         1.018602   \n",
       "3181              34         1.305212        -0.210553        -1.756361   \n",
       "3321               1        -0.169615        -0.184668        -1.201369   \n",
       "3889              48        -0.086232        -0.101835        -0.646376   \n",
       "...              ...              ...              ...              ...   \n",
       "403203785         28        -0.172978        -0.172527         0.463609   \n",
       "403234712         28        -0.172669        -0.158783        -1.201369   \n",
       "403234715         28        -0.172669        -0.158783        -1.201369   \n",
       "403235564         28        -0.172669        -0.158783        -1.201369   \n",
       "403244581         28        -0.143292        -0.158783        -1.201369   \n",
       "\n",
       "           Local_feature_4  Local_feature_5  Local_feature_6  Local_feature_7  \\\n",
       "txId                                                                            \n",
       "1076             -0.046932        -0.043875        -0.029140        -0.061584   \n",
       "2534              0.028105         0.055376         0.054722        -0.061584   \n",
       "3181             -0.121970        97.300650        -0.113002        -0.061584   \n",
       "3321             -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "3889             -0.121970        17.046997        -0.113002        -0.061584   \n",
       "...                    ...              ...              ...              ...   \n",
       "403203785        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "403234712        -0.121970        -0.063725        -0.113002        -0.061584   \n",
       "403234715        -0.121970        -0.063725        -0.113002        -0.061584   \n",
       "403235564        -0.121970        -0.063725        -0.113002        -0.061584   \n",
       "403244581        -0.121970        -0.043875        -0.113002        -0.061584   \n",
       "\n",
       "           Local_feature_8  Local_feature_9  ...    in_BTC_max   in_BTC_mean  \\\n",
       "txId                                         ...                               \n",
       "1076             -0.163591        -0.164980  ...  6.180084e-05  3.127478e-05   \n",
       "2534             -0.163572        -0.167757  ...  2.350094e-05  9.975293e-06   \n",
       "3181              1.348765         1.321754  ...  2.057192e-02  2.057192e-02   \n",
       "3321             -0.160199        -0.166062  ...  4.688075e-05  4.688076e-05   \n",
       "3889             -0.074885        -0.081943  ...  1.207336e-03  1.207336e-03   \n",
       "...                    ...              ...  ...           ...           ...   \n",
       "403203785        -0.163640        -0.169455  ...  7.368421e-08  7.369663e-08   \n",
       "403234712        -0.163323        -0.169142  ...  4.387898e-06  4.387910e-06   \n",
       "403234715        -0.163323        -0.169142  ...  4.387898e-06  4.387910e-06   \n",
       "403235564        -0.163323        -0.169142  ...  4.387898e-06  4.387910e-06   \n",
       "403244581        -0.133266        -0.139507  ...  4.132228e-04  4.132228e-04   \n",
       "\n",
       "           in_BTC_median  in_BTC_total   out_BTC_min   out_BTC_max  \\\n",
       "txId                                                                 \n",
       "1076        3.127597e-05  6.254833e-05  6.676470e-06  6.164291e-05   \n",
       "2534        5.421152e-06  2.992583e-05  9.815134e-08  2.194331e-05   \n",
       "3181        2.057192e-02  2.057192e-02  5.070912e-07  2.430414e-04   \n",
       "3321        4.688195e-05  4.687833e-05  8.301504e-05  3.244988e-05   \n",
       "3889        1.207337e-03  1.207333e-03  5.184866e-08  2.625465e-04   \n",
       "...                  ...           ...           ...           ...   \n",
       "403203785   7.488595e-08  7.126406e-08  5.282356e-08  5.251141e-08   \n",
       "403234712   4.389099e-06  4.385477e-06  2.477037e-05  4.401941e-06   \n",
       "403234715   4.389099e-06  4.385477e-06  2.477037e-05  4.401941e-06   \n",
       "403235564   4.389099e-06  4.385477e-06  2.477037e-05  4.401941e-06   \n",
       "403244581   4.132240e-04  4.132204e-04  5.242564e-05  4.068115e-04   \n",
       "\n",
       "           out_BTC_mean  out_BTC_median  out_BTC_total  class  \n",
       "txId                                                           \n",
       "1076       1.185148e-04    1.185150e-04   6.238815e-05      3  \n",
       "2534       1.622788e-05    8.509850e-06   2.990209e-05      2  \n",
       "3181       1.592931e-05    5.025955e-06   2.057192e-02      2  \n",
       "3321       8.904078e-05    8.904096e-05   4.687265e-05      3  \n",
       "3889       5.313067e-06    7.031674e-07   1.207300e-03      3  \n",
       "...                 ...             ...            ...    ...  \n",
       "403203785  1.157616e-07    1.159417e-07   6.146496e-08      3  \n",
       "403234712  1.660683e-05    1.660701e-05   4.371017e-06      3  \n",
       "403234715  1.660683e-05    1.660701e-05   4.371017e-06      3  \n",
       "403235564  1.660683e-05    1.660701e-05   4.371017e-06      3  \n",
       "403244581  7.849469e-04    7.849471e-04   4.132059e-04      3  \n",
       "\n",
       "[202804 rows x 184 columns]"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txs_features = df_txs_features.set_index('txId')\n",
    "df_txs_features.sort_index()\n",
    "\n",
    "y = df_txs_features['class']\n",
    "time_steps = df_txs_features['Time step']\n",
    "train_mask, test_mask = time_steps <= 34, time_steps > 34\n",
    "\n",
    "df_txs_features = df_txs_features.drop(columns=['class', 'Time step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 2] = 0   # licit\n",
    "y[y == 3] = 2   # unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txId\n",
       "3321         2\n",
       "11108        2\n",
       "51816        2\n",
       "68869        0\n",
       "89273        0\n",
       "            ..\n",
       "194747812    2\n",
       "194747925    2\n",
       "194748063    2\n",
       "194748070    2\n",
       "194835939    2\n",
       "Name: class, Length: 202804, dtype: int64"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "ids_from_src      = df_txs_edgelist['txId1']\n",
    "ids_from_dst      = df_txs_edgelist['txId2']\n",
    "\n",
    "all_ids = pd.unique(pd.concat([ids_from_src, ids_from_dst]))\n",
    "id_map  = {orig_id: idx for idx, orig_id in enumerate(all_ids)}\n",
    "df_txs_features.index = df_txs_features.index.map(id_map)\n",
    "\n",
    "src = df_txs_edgelist['txId1'].map(id_map).values\n",
    "dst = df_txs_edgelist['txId2'].map(id_map).values\n",
    "edge_index = torch.tensor([src, dst], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[202804, 182], edge_index=[2, 233283], y=[202804], train_mask=txId\n",
       "3321          True\n",
       "11108         True\n",
       "51816         True\n",
       "68869         True\n",
       "89273         True\n",
       "             ...  \n",
       "194747812    False\n",
       "194747925    False\n",
       "194748063    False\n",
       "194748070    False\n",
       "194835939    False\n",
       "Name: Time step, Length: 202804, dtype: bool, test_mask=txId\n",
       "3321         False\n",
       "11108        False\n",
       "51816        False\n",
       "68869        False\n",
       "89273        False\n",
       "             ...  \n",
       "194747812     True\n",
       "194747925     True\n",
       "194748063     True\n",
       "194748070     True\n",
       "194835939     True\n",
       "Name: Time step, Length: 202804, dtype: bool, node_time=[202804])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_steps = torch.tensor(time_steps.values, dtype=torch.long)\n",
    "\n",
    "x = torch.tensor(df_txs_features.values, dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, test_mask=test_mask, node_time=time_steps)\n",
    "data.y = torch.from_numpy(data.y.to_numpy().astype(int)).long()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Single Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_same_timestep_negative_edges(data: Data, neg_per_pos=1):\n",
    "    node_time = data.node_time\n",
    "    edge_index = data.edge_index\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Step 1: Build positive edge list\n",
    "    pos_edge_index = edge_index\n",
    "\n",
    "    # Step 2: Group node indices by time\n",
    "    time_to_nodes = {}\n",
    "    for t in torch.unique(node_time):\n",
    "        nodes = (node_time == t).nonzero(as_tuple=True)[0].tolist()\n",
    "        time_to_nodes[t.item()] = nodes\n",
    "\n",
    "    # Step 3: Build a set of existing edges for quick lookup\n",
    "    existing_edges = set((u.item(), v.item()) for u, v in edge_index.t())\n",
    "\n",
    "    # Step 4: Sample negative edges within each timestep\n",
    "    neg_edges = []\n",
    "    max_trials = 10 * len(nodes) ** 2  # safeguard to avoid infinite loop     \n",
    "\n",
    "    for t, nodes in time_to_nodes.items():\n",
    "        trials = 0\n",
    "        sampled = 0\n",
    "        while sampled < neg_per_pos * len(pos_edge_index[0]) and trials < max_trials:\n",
    "            u, v = random.sample(nodes, 2)\n",
    "            if (u, v) not in existing_edges and (v, u) not in existing_edges and u != v:\n",
    "                neg_edges.append((u, v))\n",
    "                sampled += 1\n",
    "            trials += 1\n",
    "\n",
    "    # Step 5: Convert to tensors\n",
    "    neg_edge_index = torch.tensor(neg_edges, dtype=torch.long).t()\n",
    "    pos_edge_label = torch.ones(pos_edge_index.size(1), dtype=torch.float)\n",
    "    neg_edge_label = torch.zeros(neg_edge_index.size(1), dtype=torch.float)\n",
    "\n",
    "    edge_label_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "    edge_label = torch.cat([pos_edge_label, neg_edge_label], dim=0)\n",
    "\n",
    "    return edge_label_index, edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1\n",
      "t = 2\n",
      "t = 3\n",
      "t = 4\n",
      "t = 5\n",
      "t = 6\n",
      "t = 7\n",
      "t = 8\n",
      "t = 9\n",
      "t = 10\n",
      "t = 11\n",
      "t = 12\n",
      "t = 13\n",
      "t = 14\n",
      "t = 15\n",
      "t = 16\n",
      "t = 17\n",
      "t = 18\n",
      "t = 19\n",
      "t = 20\n",
      "t = 21\n",
      "t = 22\n",
      "t = 23\n",
      "t = 24\n",
      "t = 25\n",
      "t = 26\n",
      "t = 27\n",
      "t = 28\n",
      "t = 29\n",
      "t = 30\n",
      "t = 31\n",
      "t = 32\n",
      "t = 33\n",
      "t = 34\n",
      "t = 35\n",
      "t = 36\n",
      "t = 37\n",
      "t = 38\n",
      "t = 39\n",
      "t = 40\n",
      "t = 41\n",
      "t = 42\n",
      "t = 43\n",
      "t = 44\n",
      "t = 45\n",
      "t = 46\n",
      "t = 47\n",
      "t = 48\n",
      "t = 49\n"
     ]
    }
   ],
   "source": [
    "edge_label_index, edge_label = sample_same_timestep_negative_edges(data, neg_per_pos=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.2728\n",
      "Epoch 002 | Loss: 1.2507\n",
      "Epoch 003 | Loss: 1.2195\n",
      "Epoch 004 | Loss: 1.1822\n",
      "Epoch 005 | Loss: 1.1425\n",
      "Epoch 006 | Loss: 1.1043\n",
      "Epoch 007 | Loss: 1.0706\n",
      "Epoch 008 | Loss: 1.0430\n",
      "Epoch 009 | Loss: 1.0221\n",
      "Epoch 010 | Loss: 1.0074\n",
      "Epoch 011 | Loss: 0.9979\n",
      "Epoch 012 | Loss: 0.9926\n",
      "Epoch 013 | Loss: 0.9902\n",
      "Epoch 014 | Loss: 0.9895\n",
      "Epoch 015 | Loss: 0.9898\n",
      "Epoch 016 | Loss: 0.9904\n",
      "Epoch 017 | Loss: 0.9908\n",
      "Epoch 018 | Loss: 0.9908\n",
      "Epoch 019 | Loss: 0.9902\n",
      "Epoch 020 | Loss: 0.9891\n",
      "Epoch 021 | Loss: 0.9876\n",
      "Epoch 022 | Loss: 0.9858\n",
      "Epoch 023 | Loss: 0.9837\n",
      "Epoch 024 | Loss: 0.9815\n",
      "Epoch 025 | Loss: 0.9793\n",
      "Epoch 026 | Loss: 0.9771\n",
      "Epoch 027 | Loss: 0.9750\n",
      "Epoch 028 | Loss: 0.9730\n",
      "Epoch 029 | Loss: 0.9711\n",
      "Epoch 030 | Loss: 0.9694\n",
      "Epoch 031 | Loss: 0.9678\n",
      "Epoch 032 | Loss: 0.9665\n",
      "Epoch 033 | Loss: 0.9652\n",
      "Epoch 034 | Loss: 0.9642\n",
      "Epoch 035 | Loss: 0.9632\n",
      "Epoch 036 | Loss: 0.9623\n",
      "Epoch 037 | Loss: 0.9615\n",
      "Epoch 038 | Loss: 0.9607\n",
      "Epoch 039 | Loss: 0.9600\n",
      "Epoch 040 | Loss: 0.9593\n",
      "Epoch 041 | Loss: 0.9585\n",
      "Epoch 042 | Loss: 0.9578\n",
      "Epoch 043 | Loss: 0.9571\n",
      "Epoch 044 | Loss: 0.9563\n",
      "Epoch 045 | Loss: 0.9556\n",
      "Epoch 046 | Loss: 0.9548\n",
      "Epoch 047 | Loss: 0.9541\n",
      "Epoch 048 | Loss: 0.9533\n",
      "Epoch 049 | Loss: 0.9526\n",
      "Epoch 050 | Loss: 0.9520\n",
      "Epoch 051 | Loss: 0.9513\n",
      "Epoch 052 | Loss: 0.9507\n",
      "Epoch 053 | Loss: 0.9501\n",
      "Epoch 054 | Loss: 0.9495\n",
      "Epoch 055 | Loss: 0.9489\n",
      "Epoch 056 | Loss: 0.9483\n",
      "Epoch 057 | Loss: 0.9477\n",
      "Epoch 058 | Loss: 0.9472\n",
      "Epoch 059 | Loss: 0.9466\n",
      "Epoch 060 | Loss: 0.9460\n",
      "Epoch 061 | Loss: 0.9455\n",
      "Epoch 062 | Loss: 0.9449\n",
      "Epoch 063 | Loss: 0.9444\n",
      "Epoch 064 | Loss: 0.9438\n",
      "Epoch 065 | Loss: 0.9433\n",
      "Epoch 066 | Loss: 0.9428\n",
      "Epoch 067 | Loss: 0.9422\n",
      "Epoch 068 | Loss: 0.9417\n",
      "Epoch 069 | Loss: 0.9412\n",
      "Epoch 070 | Loss: 0.9407\n",
      "Epoch 071 | Loss: 0.9401\n",
      "Epoch 072 | Loss: 0.9396\n",
      "Epoch 073 | Loss: 0.9391\n",
      "Epoch 074 | Loss: 0.9386\n",
      "Epoch 075 | Loss: 0.9381\n",
      "Epoch 076 | Loss: 0.9376\n",
      "Epoch 077 | Loss: 0.9371\n",
      "Epoch 078 | Loss: 0.9366\n",
      "Epoch 079 | Loss: 0.9361\n",
      "Epoch 080 | Loss: 0.9356\n",
      "Epoch 081 | Loss: 0.9351\n",
      "Epoch 082 | Loss: 0.9345\n",
      "Epoch 083 | Loss: 0.9340\n",
      "Epoch 084 | Loss: 0.9335\n",
      "Epoch 085 | Loss: 0.9330\n",
      "Epoch 086 | Loss: 0.9325\n",
      "Epoch 087 | Loss: 0.9320\n",
      "Epoch 088 | Loss: 0.9315\n",
      "Epoch 089 | Loss: 0.9310\n",
      "Epoch 090 | Loss: 0.9305\n",
      "Epoch 091 | Loss: 0.9300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[442], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m logits \u001b[38;5;241m=\u001b[39m gae_model\u001b[38;5;241m.\u001b[39mdecode(z, edge_label_index)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(logits, edge_label)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/elliptic-fork/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/elliptic-fork/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/elliptic-fork/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# 2) Encoder + Classifier\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "    \n",
    "in_feats    = data.num_node_features\n",
    "hidden_dim  = 128\n",
    "out_dim     = 64\n",
    "num_classes = int(data.y[data.y != 2].max() + 1)\n",
    "\n",
    "encoder    = GAEEncoder(in_feats, hidden_dim, out_dim)\n",
    "gae_model  = GAE(encoder)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(gae_model.parameters()),\n",
    "    lr=0.001\n",
    ")\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs+1):\n",
    "    _ = gae_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = gae_model.encode(data.x, data.edge_index)  # Full graph for encoder\n",
    "    logits = gae_model.decode(z, edge_label_index).view(-1)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, edge_label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Temporal Graph Dataset\n",
    "Extract each sub-graph for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "class PerTimestepEdgeLabelDataset(InMemoryDataset):\n",
    "    def __init__(self, data: Data, neg_sampling_ratio=1.0):\n",
    "        super().__init__(\"/tmp\")\n",
    "        self.data_list = []\n",
    "\n",
    "        for t in torch.unique(data.node_time):\n",
    "            if t >= 42:\n",
    "                break\n",
    "\n",
    "            # print(f\"t = {t}\")\n",
    "\n",
    "            subgraph_t = self.build_subgraph(data, t)\n",
    "            if subgraph_t is None:\n",
    "                continue\n",
    "                \n",
    "            train_data_t = self.random_split(subgraph_t, neg_sampling_ratio=neg_sampling_ratio)\n",
    "            \n",
    "            pos_edge_index = train_data_t.pos_edge_label_index\n",
    "            neg_edge_index = train_data_t.neg_edge_label_index\n",
    "\n",
    "            edge_label_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "            edge_label = torch.cat([\n",
    "                torch.ones(pos_edge_index.size(1), dtype=torch.float),\n",
    "                torch.zeros(neg_edge_index.size(1), dtype=torch.float)\n",
    "            ], dim=0)\n",
    "\n",
    "            data_t = Data(\n",
    "                x=subgraph_t.x,\n",
    "                edge_index=train_data_t.edge_index,\n",
    "                edge_label_index=edge_label_index,\n",
    "                edge_label=edge_label,\n",
    "                node_time=subgraph_t.node_time,\n",
    "            )\n",
    "            self.data_list.append(data_t)\n",
    "\n",
    "    def build_subgraph(self, data, time_step):\n",
    "        node_mask = data.node_time == time_step\n",
    "        if node_mask.sum() == 0:\n",
    "            return None\n",
    "\n",
    "        src, dst = data.edge_index\n",
    "        edge_mask = node_mask[src] & node_mask[dst]\n",
    "        edge_index_t = data.edge_index[:, edge_mask]\n",
    "\n",
    "        if edge_index_t.size(1) == 0:\n",
    "            # print(\"None\")\n",
    "            return None\n",
    "        \n",
    "        node_indices = node_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # 注意：x 和 node_time 都只保留時間 t \n",
    "        x_t = data.x[node_indices]\n",
    "        node_time_t = data.node_time[node_indices]\n",
    "\n",
    "        # 建圖時重新編號節點 (Optional: for cleaner subgraph)\n",
    "        index_map = torch.full((data.num_nodes,), -1, dtype=torch.long)\n",
    "        index_map[node_indices] = torch.arange(node_indices.size(0))\n",
    "        edge_index_t = index_map[edge_index_t]\n",
    "\n",
    "        return Data(\n",
    "            x=x_t,\n",
    "            edge_index=edge_index_t,    \n",
    "            num_nodes=node_indices.size(0),\n",
    "            node_time=node_time_t,\n",
    "        )\n",
    "\n",
    "    def random_split(self, data, neg_sampling_ratio=1.0):\n",
    "        splitter = RandomLinkSplit(\n",
    "            is_undirected=False,\n",
    "            split_labels=True,\n",
    "            add_negative_train_samples=True,\n",
    "            neg_sampling_ratio=neg_sampling_ratio,\n",
    "            num_val=0,\n",
    "            num_test=0,\n",
    "        )\n",
    "        train_data_t, _, _ = splitter(data)\n",
    "        return train_data_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = PerTimestepEdgeLabelDataset(data, neg_sampling_ratio=1.0)\n",
    "len(dataset)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 0.8132 | AUC: 0.6255 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 002 | Loss: 0.8132 | AUC: 0.6349 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 003 | Loss: 0.8132 | AUC: 0.6436 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 004 | Loss: 0.8131 | AUC: 0.6524 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 005 | Loss: 0.8131 | AUC: 0.6608 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 006 | Loss: 0.8130 | AUC: 0.6683 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 007 | Loss: 0.8128 | AUC: 0.6761 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 008 | Loss: 0.8126 | AUC: 0.6838 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 009 | Loss: 0.8123 | AUC: 0.6910 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 010 | Loss: 0.8119 | AUC: 0.6974 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 011 | Loss: 0.8115 | AUC: 0.7042 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 012 | Loss: 0.8109 | AUC: 0.7109 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 013 | Loss: 0.8101 | AUC: 0.7172 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 014 | Loss: 0.8092 | AUC: 0.7250 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 015 | Loss: 0.8081 | AUC: 0.7317 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 016 | Loss: 0.8067 | AUC: 0.7377 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 017 | Loss: 0.8052 | AUC: 0.7437 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 018 | Loss: 0.8034 | AUC: 0.7491 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 019 | Loss: 0.8015 | AUC: 0.7545 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 020 | Loss: 0.7992 | AUC: 0.7595 | F1: 0.6667 | Precision: 0.5000 | Recall: 1.0000\n",
      "Epoch 021 | Loss: 0.7966 | AUC: 0.7650 | F1: 0.6667 | Precision: 0.5001 | Recall: 1.0000\n",
      "Epoch 022 | Loss: 0.7938 | AUC: 0.7700 | F1: 0.6667 | Precision: 0.5001 | Recall: 1.0000\n",
      "Epoch 023 | Loss: 0.7907 | AUC: 0.7752 | F1: 0.6668 | Precision: 0.5001 | Recall: 1.0000\n",
      "Epoch 024 | Loss: 0.7872 | AUC: 0.7804 | F1: 0.6668 | Precision: 0.5001 | Recall: 0.9999\n",
      "Epoch 025 | Loss: 0.7834 | AUC: 0.7853 | F1: 0.6668 | Precision: 0.5002 | Recall: 0.9999\n",
      "Epoch 026 | Loss: 0.7793 | AUC: 0.7903 | F1: 0.6669 | Precision: 0.5003 | Recall: 0.9997\n",
      "Epoch 027 | Loss: 0.7749 | AUC: 0.7952 | F1: 0.6670 | Precision: 0.5005 | Recall: 0.9997\n",
      "Epoch 028 | Loss: 0.7702 | AUC: 0.8000 | F1: 0.6670 | Precision: 0.5005 | Recall: 0.9996\n",
      "Epoch 029 | Loss: 0.7651 | AUC: 0.8043 | F1: 0.6671 | Precision: 0.5006 | Recall: 0.9996\n",
      "Epoch 030 | Loss: 0.7598 | AUC: 0.8087 | F1: 0.6671 | Precision: 0.5006 | Recall: 0.9995\n",
      "Epoch 031 | Loss: 0.7541 | AUC: 0.8135 | F1: 0.6672 | Precision: 0.5008 | Recall: 0.9992\n",
      "Epoch 032 | Loss: 0.7482 | AUC: 0.8176 | F1: 0.6672 | Precision: 0.5008 | Recall: 0.9990\n",
      "Epoch 033 | Loss: 0.7421 | AUC: 0.8219 | F1: 0.6675 | Precision: 0.5011 | Recall: 0.9990\n",
      "Epoch 034 | Loss: 0.7358 | AUC: 0.8260 | F1: 0.6675 | Precision: 0.5012 | Recall: 0.9990\n",
      "Epoch 035 | Loss: 0.7295 | AUC: 0.8300 | F1: 0.6677 | Precision: 0.5014 | Recall: 0.9990\n",
      "Epoch 036 | Loss: 0.7232 | AUC: 0.8337 | F1: 0.6679 | Precision: 0.5016 | Recall: 0.9990\n",
      "Epoch 037 | Loss: 0.7170 | AUC: 0.8372 | F1: 0.6681 | Precision: 0.5019 | Recall: 0.9990\n",
      "Epoch 038 | Loss: 0.7110 | AUC: 0.8402 | F1: 0.6683 | Precision: 0.5022 | Recall: 0.9989\n",
      "Epoch 039 | Loss: 0.7052 | AUC: 0.8432 | F1: 0.6685 | Precision: 0.5024 | Recall: 0.9988\n",
      "Epoch 040 | Loss: 0.6997 | AUC: 0.8459 | F1: 0.6687 | Precision: 0.5026 | Recall: 0.9989\n",
      "Epoch 041 | Loss: 0.6946 | AUC: 0.8479 | F1: 0.6690 | Precision: 0.5029 | Recall: 0.9988\n",
      "Epoch 042 | Loss: 0.6898 | AUC: 0.8493 | F1: 0.6694 | Precision: 0.5033 | Recall: 0.9988\n",
      "Epoch 043 | Loss: 0.6853 | AUC: 0.8505 | F1: 0.6697 | Precision: 0.5038 | Recall: 0.9986\n",
      "Epoch 044 | Loss: 0.6814 | AUC: 0.8517 | F1: 0.6700 | Precision: 0.5041 | Recall: 0.9986\n",
      "Epoch 045 | Loss: 0.6780 | AUC: 0.8523 | F1: 0.6704 | Precision: 0.5045 | Recall: 0.9988\n",
      "Epoch 046 | Loss: 0.6750 | AUC: 0.8527 | F1: 0.6706 | Precision: 0.5048 | Recall: 0.9985\n",
      "Epoch 047 | Loss: 0.6724 | AUC: 0.8526 | F1: 0.6707 | Precision: 0.5050 | Recall: 0.9985\n",
      "Epoch 048 | Loss: 0.6700 | AUC: 0.8525 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 049 | Loss: 0.6681 | AUC: 0.8519 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9986\n",
      "Epoch 050 | Loss: 0.6664 | AUC: 0.8513 | F1: 0.6715 | Precision: 0.5059 | Recall: 0.9985\n",
      "Epoch 051 | Loss: 0.6649 | AUC: 0.8506 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 052 | Loss: 0.6637 | AUC: 0.8499 | F1: 0.6717 | Precision: 0.5062 | Recall: 0.9979\n",
      "Epoch 053 | Loss: 0.6626 | AUC: 0.8492 | F1: 0.6720 | Precision: 0.5066 | Recall: 0.9979\n",
      "Epoch 054 | Loss: 0.6617 | AUC: 0.8486 | F1: 0.6723 | Precision: 0.5068 | Recall: 0.9979\n",
      "Epoch 055 | Loss: 0.6609 | AUC: 0.8481 | F1: 0.6722 | Precision: 0.5068 | Recall: 0.9978\n",
      "Epoch 056 | Loss: 0.6602 | AUC: 0.8478 | F1: 0.6723 | Precision: 0.5069 | Recall: 0.9978\n",
      "Epoch 057 | Loss: 0.6594 | AUC: 0.8475 | F1: 0.6724 | Precision: 0.5070 | Recall: 0.9978\n",
      "Epoch 058 | Loss: 0.6587 | AUC: 0.8474 | F1: 0.6723 | Precision: 0.5070 | Recall: 0.9975\n",
      "Epoch 059 | Loss: 0.6579 | AUC: 0.8475 | F1: 0.6724 | Precision: 0.5072 | Recall: 0.9975\n",
      "Epoch 060 | Loss: 0.6573 | AUC: 0.8476 | F1: 0.6725 | Precision: 0.5072 | Recall: 0.9977\n",
      "Epoch 061 | Loss: 0.6566 | AUC: 0.8478 | F1: 0.6725 | Precision: 0.5073 | Recall: 0.9975\n",
      "Epoch 062 | Loss: 0.6560 | AUC: 0.8481 | F1: 0.6725 | Precision: 0.5073 | Recall: 0.9975\n",
      "Epoch 063 | Loss: 0.6554 | AUC: 0.8484 | F1: 0.6726 | Precision: 0.5073 | Recall: 0.9977\n",
      "Epoch 064 | Loss: 0.6548 | AUC: 0.8488 | F1: 0.6725 | Precision: 0.5072 | Recall: 0.9977\n",
      "Epoch 065 | Loss: 0.6543 | AUC: 0.8492 | F1: 0.6724 | Precision: 0.5071 | Recall: 0.9975\n",
      "Epoch 066 | Loss: 0.6538 | AUC: 0.8496 | F1: 0.6724 | Precision: 0.5071 | Recall: 0.9977\n",
      "Epoch 067 | Loss: 0.6533 | AUC: 0.8501 | F1: 0.6723 | Precision: 0.5069 | Recall: 0.9977\n",
      "Epoch 068 | Loss: 0.6528 | AUC: 0.8506 | F1: 0.6721 | Precision: 0.5068 | Recall: 0.9975\n",
      "Epoch 069 | Loss: 0.6523 | AUC: 0.8511 | F1: 0.6720 | Precision: 0.5067 | Recall: 0.9975\n",
      "Epoch 070 | Loss: 0.6519 | AUC: 0.8516 | F1: 0.6719 | Precision: 0.5066 | Recall: 0.9975\n",
      "Epoch 071 | Loss: 0.6515 | AUC: 0.8522 | F1: 0.6718 | Precision: 0.5064 | Recall: 0.9975\n",
      "Epoch 072 | Loss: 0.6511 | AUC: 0.8528 | F1: 0.6717 | Precision: 0.5062 | Recall: 0.9978\n",
      "Epoch 073 | Loss: 0.6507 | AUC: 0.8533 | F1: 0.6716 | Precision: 0.5061 | Recall: 0.9978\n",
      "Epoch 074 | Loss: 0.6503 | AUC: 0.8539 | F1: 0.6716 | Precision: 0.5061 | Recall: 0.9978\n",
      "Epoch 075 | Loss: 0.6499 | AUC: 0.8545 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9978\n",
      "Epoch 076 | Loss: 0.6495 | AUC: 0.8552 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9978\n",
      "Epoch 077 | Loss: 0.6491 | AUC: 0.8558 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9978\n",
      "Epoch 078 | Loss: 0.6487 | AUC: 0.8564 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9979\n",
      "Epoch 079 | Loss: 0.6483 | AUC: 0.8571 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9979\n",
      "Epoch 080 | Loss: 0.6479 | AUC: 0.8578 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9979\n",
      "Epoch 081 | Loss: 0.6475 | AUC: 0.8585 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9981\n",
      "Epoch 082 | Loss: 0.6471 | AUC: 0.8592 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9981\n",
      "Epoch 083 | Loss: 0.6468 | AUC: 0.8599 | F1: 0.6715 | Precision: 0.5059 | Recall: 0.9984\n",
      "Epoch 084 | Loss: 0.6464 | AUC: 0.8605 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9984\n",
      "Epoch 085 | Loss: 0.6460 | AUC: 0.8612 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9984\n",
      "Epoch 086 | Loss: 0.6456 | AUC: 0.8618 | F1: 0.6715 | Precision: 0.5059 | Recall: 0.9984\n",
      "Epoch 087 | Loss: 0.6453 | AUC: 0.8625 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9984\n",
      "Epoch 088 | Loss: 0.6449 | AUC: 0.8631 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9984\n",
      "Epoch 089 | Loss: 0.6446 | AUC: 0.8637 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9984\n",
      "Epoch 090 | Loss: 0.6442 | AUC: 0.8644 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9984\n",
      "Epoch 091 | Loss: 0.6439 | AUC: 0.8650 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9984\n",
      "Epoch 092 | Loss: 0.6436 | AUC: 0.8656 | F1: 0.6712 | Precision: 0.5056 | Recall: 0.9984\n",
      "Epoch 093 | Loss: 0.6433 | AUC: 0.8661 | F1: 0.6712 | Precision: 0.5055 | Recall: 0.9984\n",
      "Epoch 094 | Loss: 0.6430 | AUC: 0.8666 | F1: 0.6711 | Precision: 0.5055 | Recall: 0.9984\n",
      "Epoch 095 | Loss: 0.6427 | AUC: 0.8671 | F1: 0.6712 | Precision: 0.5055 | Recall: 0.9985\n",
      "Epoch 096 | Loss: 0.6424 | AUC: 0.8677 | F1: 0.6711 | Precision: 0.5054 | Recall: 0.9985\n",
      "Epoch 097 | Loss: 0.6421 | AUC: 0.8682 | F1: 0.6711 | Precision: 0.5054 | Recall: 0.9985\n",
      "Epoch 098 | Loss: 0.6418 | AUC: 0.8687 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 099 | Loss: 0.6415 | AUC: 0.8692 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 100 | Loss: 0.6412 | AUC: 0.8697 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 101 | Loss: 0.6409 | AUC: 0.8703 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 102 | Loss: 0.6406 | AUC: 0.8707 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 103 | Loss: 0.6403 | AUC: 0.8712 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 104 | Loss: 0.6401 | AUC: 0.8717 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 105 | Loss: 0.6398 | AUC: 0.8722 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 106 | Loss: 0.6394 | AUC: 0.8727 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 107 | Loss: 0.6392 | AUC: 0.8732 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9988\n",
      "Epoch 108 | Loss: 0.6389 | AUC: 0.8736 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9988\n",
      "Epoch 109 | Loss: 0.6386 | AUC: 0.8739 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9988\n",
      "Epoch 110 | Loss: 0.6384 | AUC: 0.8744 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9988\n",
      "Epoch 111 | Loss: 0.6381 | AUC: 0.8748 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 112 | Loss: 0.6378 | AUC: 0.8752 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9989\n",
      "Epoch 113 | Loss: 0.6375 | AUC: 0.8757 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9989\n",
      "Epoch 114 | Loss: 0.6372 | AUC: 0.8761 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 115 | Loss: 0.6369 | AUC: 0.8765 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9989\n",
      "Epoch 116 | Loss: 0.6366 | AUC: 0.8769 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9989\n",
      "Epoch 117 | Loss: 0.6363 | AUC: 0.8772 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 118 | Loss: 0.6360 | AUC: 0.8776 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 119 | Loss: 0.6357 | AUC: 0.8779 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 120 | Loss: 0.6355 | AUC: 0.8783 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 121 | Loss: 0.6353 | AUC: 0.8786 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 122 | Loss: 0.6351 | AUC: 0.8789 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 123 | Loss: 0.6348 | AUC: 0.8792 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 124 | Loss: 0.6346 | AUC: 0.8794 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 125 | Loss: 0.6344 | AUC: 0.8796 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 126 | Loss: 0.6342 | AUC: 0.8798 | F1: 0.6712 | Precision: 0.5053 | Recall: 0.9990\n",
      "Epoch 127 | Loss: 0.6339 | AUC: 0.8800 | F1: 0.6711 | Precision: 0.5052 | Recall: 0.9990\n",
      "Epoch 128 | Loss: 0.6337 | AUC: 0.8802 | F1: 0.6711 | Precision: 0.5052 | Recall: 0.9990\n",
      "Epoch 129 | Loss: 0.6335 | AUC: 0.8804 | F1: 0.6711 | Precision: 0.5052 | Recall: 0.9990\n",
      "Epoch 130 | Loss: 0.6333 | AUC: 0.8806 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9990\n",
      "Epoch 131 | Loss: 0.6331 | AUC: 0.8808 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9990\n",
      "Epoch 132 | Loss: 0.6329 | AUC: 0.8810 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9990\n",
      "Epoch 133 | Loss: 0.6327 | AUC: 0.8812 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 134 | Loss: 0.6325 | AUC: 0.8813 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 135 | Loss: 0.6323 | AUC: 0.8815 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 136 | Loss: 0.6321 | AUC: 0.8817 | F1: 0.6711 | Precision: 0.5053 | Recall: 0.9989\n",
      "Epoch 137 | Loss: 0.6319 | AUC: 0.8818 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 138 | Loss: 0.6317 | AUC: 0.8820 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 139 | Loss: 0.6315 | AUC: 0.8822 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 140 | Loss: 0.6313 | AUC: 0.8823 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 141 | Loss: 0.6311 | AUC: 0.8825 | F1: 0.6712 | Precision: 0.5054 | Recall: 0.9989\n",
      "Epoch 142 | Loss: 0.6309 | AUC: 0.8826 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9989\n",
      "Epoch 143 | Loss: 0.6307 | AUC: 0.8828 | F1: 0.6710 | Precision: 0.5051 | Recall: 0.9989\n",
      "Epoch 144 | Loss: 0.6305 | AUC: 0.8830 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9989\n",
      "Epoch 145 | Loss: 0.6303 | AUC: 0.8833 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9989\n",
      "Epoch 146 | Loss: 0.6301 | AUC: 0.8835 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9989\n",
      "Epoch 147 | Loss: 0.6299 | AUC: 0.8837 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9989\n",
      "Epoch 148 | Loss: 0.6298 | AUC: 0.8839 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9989\n",
      "Epoch 149 | Loss: 0.6296 | AUC: 0.8840 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9989\n",
      "Epoch 150 | Loss: 0.6294 | AUC: 0.8841 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 151 | Loss: 0.6292 | AUC: 0.8843 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 152 | Loss: 0.6290 | AUC: 0.8845 | F1: 0.6707 | Precision: 0.5049 | Recall: 0.9988\n",
      "Epoch 153 | Loss: 0.6289 | AUC: 0.8847 | F1: 0.6707 | Precision: 0.5049 | Recall: 0.9988\n",
      "Epoch 154 | Loss: 0.6287 | AUC: 0.8849 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 155 | Loss: 0.6285 | AUC: 0.8851 | F1: 0.6708 | Precision: 0.5049 | Recall: 0.9988\n",
      "Epoch 156 | Loss: 0.6283 | AUC: 0.8852 | F1: 0.6708 | Precision: 0.5049 | Recall: 0.9988\n",
      "Epoch 157 | Loss: 0.6282 | AUC: 0.8854 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 158 | Loss: 0.6280 | AUC: 0.8856 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 159 | Loss: 0.6278 | AUC: 0.8858 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 160 | Loss: 0.6276 | AUC: 0.8860 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 161 | Loss: 0.6275 | AUC: 0.8861 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 162 | Loss: 0.6273 | AUC: 0.8863 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 163 | Loss: 0.6271 | AUC: 0.8865 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 164 | Loss: 0.6269 | AUC: 0.8866 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 165 | Loss: 0.6268 | AUC: 0.8868 | F1: 0.6709 | Precision: 0.5050 | Recall: 0.9988\n",
      "Epoch 166 | Loss: 0.6266 | AUC: 0.8869 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 167 | Loss: 0.6265 | AUC: 0.8871 | F1: 0.6710 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 168 | Loss: 0.6263 | AUC: 0.8873 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 169 | Loss: 0.6261 | AUC: 0.8875 | F1: 0.6710 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 170 | Loss: 0.6260 | AUC: 0.8876 | F1: 0.6710 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 171 | Loss: 0.6258 | AUC: 0.8878 | F1: 0.6710 | Precision: 0.5051 | Recall: 0.9988\n",
      "Epoch 172 | Loss: 0.6256 | AUC: 0.8880 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9988\n",
      "Epoch 173 | Loss: 0.6255 | AUC: 0.8882 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9988\n",
      "Epoch 174 | Loss: 0.6253 | AUC: 0.8884 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9988\n",
      "Epoch 175 | Loss: 0.6251 | AUC: 0.8885 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 176 | Loss: 0.6250 | AUC: 0.8887 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 177 | Loss: 0.6248 | AUC: 0.8889 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9986\n",
      "Epoch 178 | Loss: 0.6246 | AUC: 0.8890 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 179 | Loss: 0.6244 | AUC: 0.8892 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 180 | Loss: 0.6243 | AUC: 0.8894 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 181 | Loss: 0.6241 | AUC: 0.8896 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 182 | Loss: 0.6240 | AUC: 0.8897 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 183 | Loss: 0.6238 | AUC: 0.8899 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9986\n",
      "Epoch 184 | Loss: 0.6236 | AUC: 0.8901 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 185 | Loss: 0.6235 | AUC: 0.8902 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 186 | Loss: 0.6233 | AUC: 0.8903 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 187 | Loss: 0.6231 | AUC: 0.8905 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 188 | Loss: 0.6230 | AUC: 0.8907 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 189 | Loss: 0.6228 | AUC: 0.8909 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 190 | Loss: 0.6226 | AUC: 0.8911 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 191 | Loss: 0.6224 | AUC: 0.8913 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 192 | Loss: 0.6223 | AUC: 0.8915 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 193 | Loss: 0.6221 | AUC: 0.8916 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 194 | Loss: 0.6220 | AUC: 0.8918 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 195 | Loss: 0.6218 | AUC: 0.8920 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 196 | Loss: 0.6216 | AUC: 0.8921 | F1: 0.6709 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 197 | Loss: 0.6215 | AUC: 0.8923 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9985\n",
      "Epoch 198 | Loss: 0.6213 | AUC: 0.8923 | F1: 0.6708 | Precision: 0.5050 | Recall: 0.9985\n",
      "Epoch 199 | Loss: 0.6212 | AUC: 0.8924 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 200 | Loss: 0.6210 | AUC: 0.8926 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 201 | Loss: 0.6208 | AUC: 0.8928 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 202 | Loss: 0.6207 | AUC: 0.8930 | F1: 0.6708 | Precision: 0.5051 | Recall: 0.9985\n",
      "Epoch 203 | Loss: 0.6205 | AUC: 0.8931 | F1: 0.6709 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 204 | Loss: 0.6203 | AUC: 0.8933 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 205 | Loss: 0.6202 | AUC: 0.8935 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 206 | Loss: 0.6200 | AUC: 0.8937 | F1: 0.6710 | Precision: 0.5052 | Recall: 0.9985\n",
      "Epoch 207 | Loss: 0.6198 | AUC: 0.8938 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 208 | Loss: 0.6197 | AUC: 0.8940 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 209 | Loss: 0.6195 | AUC: 0.8942 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 210 | Loss: 0.6194 | AUC: 0.8943 | F1: 0.6710 | Precision: 0.5053 | Recall: 0.9985\n",
      "Epoch 211 | Loss: 0.6192 | AUC: 0.8945 | F1: 0.6711 | Precision: 0.5054 | Recall: 0.9985\n",
      "Epoch 212 | Loss: 0.6190 | AUC: 0.8946 | F1: 0.6712 | Precision: 0.5055 | Recall: 0.9985\n",
      "Epoch 213 | Loss: 0.6189 | AUC: 0.8948 | F1: 0.6712 | Precision: 0.5055 | Recall: 0.9985\n",
      "Epoch 214 | Loss: 0.6187 | AUC: 0.8949 | F1: 0.6712 | Precision: 0.5055 | Recall: 0.9985\n",
      "Epoch 215 | Loss: 0.6186 | AUC: 0.8951 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 216 | Loss: 0.6184 | AUC: 0.8952 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 217 | Loss: 0.6183 | AUC: 0.8954 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 218 | Loss: 0.6181 | AUC: 0.8955 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 219 | Loss: 0.6179 | AUC: 0.8957 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 220 | Loss: 0.6178 | AUC: 0.8958 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 221 | Loss: 0.6176 | AUC: 0.8959 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 222 | Loss: 0.6175 | AUC: 0.8961 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 223 | Loss: 0.6173 | AUC: 0.8961 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 224 | Loss: 0.6172 | AUC: 0.8963 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 225 | Loss: 0.6170 | AUC: 0.8964 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 226 | Loss: 0.6169 | AUC: 0.8965 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9985\n",
      "Epoch 227 | Loss: 0.6167 | AUC: 0.8965 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9985\n",
      "Epoch 228 | Loss: 0.6166 | AUC: 0.8967 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9985\n",
      "Epoch 229 | Loss: 0.6165 | AUC: 0.8968 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9985\n",
      "Epoch 230 | Loss: 0.6163 | AUC: 0.8969 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9985\n",
      "Epoch 231 | Loss: 0.6162 | AUC: 0.8970 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9984\n",
      "Epoch 232 | Loss: 0.6160 | AUC: 0.8971 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9984\n",
      "Epoch 233 | Loss: 0.6159 | AUC: 0.8972 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9984\n",
      "Epoch 234 | Loss: 0.6157 | AUC: 0.8974 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9984\n",
      "Epoch 235 | Loss: 0.6156 | AUC: 0.8975 | F1: 0.6712 | Precision: 0.5056 | Recall: 0.9982\n",
      "Epoch 236 | Loss: 0.6154 | AUC: 0.8975 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9982\n",
      "Epoch 237 | Loss: 0.6153 | AUC: 0.8976 | F1: 0.6713 | Precision: 0.5056 | Recall: 0.9982\n",
      "Epoch 238 | Loss: 0.6152 | AUC: 0.8977 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 239 | Loss: 0.6150 | AUC: 0.8978 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 240 | Loss: 0.6149 | AUC: 0.8979 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 241 | Loss: 0.6147 | AUC: 0.8981 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 242 | Loss: 0.6146 | AUC: 0.8982 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 243 | Loss: 0.6145 | AUC: 0.8982 | F1: 0.6713 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 244 | Loss: 0.6143 | AUC: 0.8983 | F1: 0.6714 | Precision: 0.5057 | Recall: 0.9982\n",
      "Epoch 245 | Loss: 0.6142 | AUC: 0.8984 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9982\n",
      "Epoch 246 | Loss: 0.6140 | AUC: 0.8985 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9982\n",
      "Epoch 247 | Loss: 0.6139 | AUC: 0.8986 | F1: 0.6714 | Precision: 0.5058 | Recall: 0.9981\n",
      "Epoch 248 | Loss: 0.6138 | AUC: 0.8987 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9981\n",
      "Epoch 249 | Loss: 0.6136 | AUC: 0.8988 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9982\n",
      "Epoch 250 | Loss: 0.6135 | AUC: 0.8989 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9982\n",
      "Epoch 251 | Loss: 0.6134 | AUC: 0.8990 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9982\n",
      "Epoch 252 | Loss: 0.6132 | AUC: 0.8991 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9982\n",
      "Epoch 253 | Loss: 0.6131 | AUC: 0.8992 | F1: 0.6714 | Precision: 0.5059 | Recall: 0.9982\n",
      "Epoch 254 | Loss: 0.6130 | AUC: 0.8993 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 255 | Loss: 0.6129 | AUC: 0.8994 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 256 | Loss: 0.6127 | AUC: 0.8995 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 257 | Loss: 0.6126 | AUC: 0.8996 | F1: 0.6715 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 258 | Loss: 0.6125 | AUC: 0.8997 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 259 | Loss: 0.6123 | AUC: 0.8998 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 260 | Loss: 0.6122 | AUC: 0.8999 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 261 | Loss: 0.6121 | AUC: 0.8999 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 262 | Loss: 0.6120 | AUC: 0.9000 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 263 | Loss: 0.6118 | AUC: 0.9001 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 264 | Loss: 0.6117 | AUC: 0.9002 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 265 | Loss: 0.6116 | AUC: 0.9003 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 266 | Loss: 0.6115 | AUC: 0.9004 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 267 | Loss: 0.6113 | AUC: 0.9004 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 268 | Loss: 0.6112 | AUC: 0.9005 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 269 | Loss: 0.6111 | AUC: 0.9006 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 270 | Loss: 0.6110 | AUC: 0.9007 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 271 | Loss: 0.6109 | AUC: 0.9008 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 272 | Loss: 0.6107 | AUC: 0.9009 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 273 | Loss: 0.6106 | AUC: 0.9010 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 274 | Loss: 0.6105 | AUC: 0.9011 | F1: 0.6716 | Precision: 0.5060 | Recall: 0.9982\n",
      "Epoch 275 | Loss: 0.6104 | AUC: 0.9011 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 276 | Loss: 0.6103 | AUC: 0.9012 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 277 | Loss: 0.6101 | AUC: 0.9013 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 278 | Loss: 0.6100 | AUC: 0.9013 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 279 | Loss: 0.6099 | AUC: 0.9014 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 280 | Loss: 0.6098 | AUC: 0.9015 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9982\n",
      "Epoch 281 | Loss: 0.6097 | AUC: 0.9016 | F1: 0.6716 | Precision: 0.5061 | Recall: 0.9981\n",
      "Epoch 282 | Loss: 0.6095 | AUC: 0.9016 | F1: 0.6716 | Precision: 0.5061 | Recall: 0.9981\n",
      "Epoch 283 | Loss: 0.6094 | AUC: 0.9017 | F1: 0.6717 | Precision: 0.5061 | Recall: 0.9981\n",
      "Epoch 284 | Loss: 0.6093 | AUC: 0.9018 | F1: 0.6717 | Precision: 0.5062 | Recall: 0.9981\n",
      "Epoch 285 | Loss: 0.6092 | AUC: 0.9019 | F1: 0.6717 | Precision: 0.5062 | Recall: 0.9981\n",
      "Epoch 286 | Loss: 0.6091 | AUC: 0.9020 | F1: 0.6718 | Precision: 0.5063 | Recall: 0.9981\n",
      "Epoch 287 | Loss: 0.6090 | AUC: 0.9020 | F1: 0.6718 | Precision: 0.5063 | Recall: 0.9981\n",
      "Epoch 288 | Loss: 0.6088 | AUC: 0.9021 | F1: 0.6718 | Precision: 0.5063 | Recall: 0.9981\n",
      "Epoch 289 | Loss: 0.6087 | AUC: 0.9022 | F1: 0.6718 | Precision: 0.5063 | Recall: 0.9981\n",
      "Epoch 290 | Loss: 0.6086 | AUC: 0.9023 | F1: 0.6718 | Precision: 0.5063 | Recall: 0.9981\n",
      "Epoch 291 | Loss: 0.6085 | AUC: 0.9024 | F1: 0.6719 | Precision: 0.5064 | Recall: 0.9981\n",
      "Epoch 292 | Loss: 0.6084 | AUC: 0.9024 | F1: 0.6719 | Precision: 0.5064 | Recall: 0.9981\n",
      "Epoch 293 | Loss: 0.6083 | AUC: 0.9025 | F1: 0.6720 | Precision: 0.5065 | Recall: 0.9981\n",
      "Epoch 294 | Loss: 0.6081 | AUC: 0.9026 | F1: 0.6720 | Precision: 0.5066 | Recall: 0.9981\n",
      "Epoch 295 | Loss: 0.6080 | AUC: 0.9027 | F1: 0.6720 | Precision: 0.5066 | Recall: 0.9979\n",
      "Epoch 296 | Loss: 0.6079 | AUC: 0.9028 | F1: 0.6721 | Precision: 0.5067 | Recall: 0.9979\n",
      "Epoch 297 | Loss: 0.6078 | AUC: 0.9028 | F1: 0.6721 | Precision: 0.5067 | Recall: 0.9979\n",
      "Epoch 298 | Loss: 0.6077 | AUC: 0.9029 | F1: 0.6722 | Precision: 0.5067 | Recall: 0.9979\n",
      "Epoch 299 | Loss: 0.6076 | AUC: 0.9029 | F1: 0.6722 | Precision: 0.5067 | Recall: 0.9979\n",
      "Epoch 300 | Loss: 0.6074 | AUC: 0.9030 | F1: 0.6722 | Precision: 0.5068 | Recall: 0.9979\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Define Encoder\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Model & Optimizer\n",
    "in_feats = data.num_node_features\n",
    "hidden_dim = 256\n",
    "out_dim = 128\n",
    "encoder = GAEEncoder(in_feats, hidden_dim, out_dim)\n",
    "gae_model = GAE(encoder)\n",
    "optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.0001)\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Get training subgraph (time step 1)\n",
    "train_batch = dataset[0]\n",
    "\n",
    "# Tracking metrics\n",
    "loss_list, auc_list, f1_list, precision_list, recall_list = [], [], [], [], []\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(1, epochs + 1):\n",
    "    _ = gae_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    z = gae_model.encode(train_batch.x.squeeze(0), train_batch.edge_index.squeeze(0))\n",
    "    edge_label_index = train_batch.edge_label_index.squeeze(0)\n",
    "    edge_label = train_batch.edge_label.squeeze(0).float()\n",
    "    logits = gae_model.decode(z, edge_label_index).view(-1)\n",
    "\n",
    "    # Loss\n",
    "    loss = bce_loss(logits, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        probs = torch.sigmoid(logits).cpu()\n",
    "        preds = (probs > 0.5).float()\n",
    "        labels = edge_label.cpu()\n",
    "\n",
    "        auc = roc_auc_score(labels, probs)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "\n",
    "        # Save metrics\n",
    "        loss_list.append(loss.item())\n",
    "        auc_list.append(auc)\n",
    "        f1_list.append(f1)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | AUC: {auc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a32f12d0>]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a32f0460>]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a32f1690>]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a32f1990>]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5a32f1c90>]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Metric Value')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Metrics Over Epochs')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x112c03760>"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(loss_list, label='Loss')\n",
    "plt.plot(auc_list, label='AUC')\n",
    "plt.plot(f1_list, label='F1')\n",
    "plt.plot(precision_list, label='Precision')\n",
    "plt.plot(recall_list, label='Recall')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Training Metrics Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min score: 0.5000, Max score: 0.7311\n",
      "Fraction > 0.5: 0.9846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.070e+02, 1.200e+01, 2.000e+00, 8.000e+00, 3.000e+00, 4.000e+00,\n",
       "        4.000e+00, 3.000e+00, 6.000e+00, 2.000e+00, 5.000e+00, 4.000e+00,\n",
       "        1.000e+00, 6.000e+00, 4.000e+00, 5.000e+00, 6.000e+00, 5.000e+00,\n",
       "        9.000e+00, 4.000e+00, 5.000e+00, 7.000e+00, 3.000e+00, 6.000e+00,\n",
       "        5.000e+00, 7.000e+00, 3.000e+00, 4.000e+00, 4.000e+00, 2.000e+00,\n",
       "        9.000e+00, 8.000e+00, 1.200e+01, 1.000e+01, 1.600e+01, 1.500e+01,\n",
       "        1.300e+01, 1.500e+01, 2.500e+01, 2.300e+01, 3.800e+01, 3.100e+01,\n",
       "        3.300e+01, 5.400e+01, 7.800e+01, 7.800e+01, 1.120e+02, 1.460e+02,\n",
       "        2.890e+02, 6.066e+03]),\n",
       " array([0.5       , 0.50462115, 0.50924236, 0.5138635 , 0.51848471,\n",
       "        0.52310586, 0.52772701, 0.53234822, 0.53696936, 0.54159057,\n",
       "        0.54621172, 0.55083287, 0.55545408, 0.56007522, 0.56469643,\n",
       "        0.56931758, 0.57393873, 0.57855994, 0.58318108, 0.58780229,\n",
       "        0.59242344, 0.59704459, 0.60166579, 0.60628694, 0.61090815,\n",
       "        0.6155293 , 0.62015045, 0.62477165, 0.6293928 , 0.63401401,\n",
       "        0.63863516, 0.64325631, 0.64787751, 0.65249866, 0.65711987,\n",
       "        0.66174102, 0.66636217, 0.67098337, 0.67560452, 0.68022573,\n",
       "        0.68484688, 0.68946803, 0.69408923, 0.69871038, 0.70333159,\n",
       "        0.70795274, 0.71257389, 0.71719509, 0.72181624, 0.72643745,\n",
       "        0.7310586 ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([2409.,  446.,  360.,  271.,  206.,  190.,  161.,  127.,   96.,\n",
       "         105.,   82.,   74.,   83.,   58.,   52.,   43.,   64.,   47.,\n",
       "          45.,   49.,   36.,   40.,   27.,   37.,   33.,   28.,   26.,\n",
       "          28.,   38.,   24.,   26.,   32.,   29.,   21.,   27.,   25.,\n",
       "          30.,   22.,   30.,   33.,   23.,   36.,   35.,   37.,   41.,\n",
       "          46.,   65.,   92.,  147., 1235.]),\n",
       " array([0.5       , 0.50462115, 0.50924236, 0.5138635 , 0.51848471,\n",
       "        0.52310586, 0.52772701, 0.53234822, 0.53696936, 0.54159057,\n",
       "        0.54621172, 0.55083287, 0.55545408, 0.56007522, 0.56469643,\n",
       "        0.56931758, 0.57393873, 0.57855994, 0.58318108, 0.58780229,\n",
       "        0.59242344, 0.59704459, 0.60166579, 0.60628694, 0.61090815,\n",
       "        0.6155293 , 0.62015045, 0.62477165, 0.6293928 , 0.63401401,\n",
       "        0.63863516, 0.64325631, 0.64787751, 0.65249866, 0.65711987,\n",
       "        0.66174102, 0.66636217, 0.67098337, 0.67560452, 0.68022573,\n",
       "        0.68484688, 0.68946803, 0.69408923, 0.69871038, 0.70333159,\n",
       "        0.70795274, 0.71257389, 0.71719509, 0.72181624, 0.72643745,\n",
       "        0.7310586 ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Predicted Score')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Score Distribution')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11174ea70>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHFCAYAAAAT5Oa6AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUqNJREFUeJzt3XtcVNXeP/DPCMNwHwXlJmigCHjHG6B5RfASoXk6HKVIO2iWeSH10TyWgBkklZc0ScnAo5h2kXPUjAOWWiaimZQKqSVeOIKYIYgiDLB+f/Rjn8bhLjDI/rxfL17P2Wt/Z+21WE58nn2ZUQghBIiIiIhkrJ2+B0BERESkbwxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREMpCeno6nnnoKXbp0gUqlgq2tLXx8fLBo0SJ9D63BIiIioFAopB9TU1M4Ojpi3Lhx2LBhA+7cuaPzmhkzZuCxxx5r0HGuX7+OiIgIZGRkNOh11R1LoVBg7ty5DeqnLps2bUJCQoJO++XLl6FQKKrdR0Q1YyAiauO++OILDB06FEVFRYiJiUFKSgrWr1+PYcOGYffu3foeXqMlJycjLS0NycnJeOedd9ClSxcsWbIEvXr1wo8//qhV+/rrryMpKalB/V+/fh2RkZENDkSNOVZj1BSI7O3tkZaWhieeeKLZx0DUlhjqewBE1LxiYmLg7OyM//znPzA0/N9bfurUqYiJiWnRsdy7dw+mpqZN0tfAgQPRsWNHaXvq1KmYO3cuRo4cicDAQFy4cAEqlQoA0K1btyY5Zm2q5tYSx6qNSqWCt7e3XsdA9CjiGSKiNu7WrVvo2LGjVhiq0q6d7n8Cdu7cCR8fH5ibm8Pc3Bz9+/fH1q1btWo++ugj9OvXD8bGxrCyssJTTz2FrKwsrZoZM2bA3NwcZ86cgb+/PywsLODr6wsAKCsrw6pVq+Du7g6VSoVOnTrh+eefx82bNx9qrv369cPy5ctx9epVrbNf1V3G+vTTT+Hl5QW1Wg1TU1O4uLjg73//OwDg8OHDGDx4MADg+eefly7PRURE1Dm32i7Pbd68GT169IBKpULPnj2xa9curf1VlwMflJCQAIVCgcuXLwMAHnvsMZw7dw5HjhyRxlZ1zJoumR09ehS+vr6wsLCAqakphg4dii+++KLa4xw6dAgvvfQSOnbsCGtra0yZMgXXr1+vdk5EbQUDEVEb5+Pjg/T0dMyfPx/p6enQaDQ11q5YsQLPPPMMHBwckJCQgKSkJEyfPh1XrlyRaqKjoxEaGopevXphz549WL9+PX766Sf4+Pjg4sWLWv2VlZUhMDAQY8aMwb///W9ERkaisrISkyZNwltvvYXg4GB88cUXeOutt5CamopRo0ahpKTkoeYbGBgIAPjmm29qrElLS8Pf/vY3uLi4YNeuXfjiiy+wYsUKlJeXAwAGDBiA+Ph4AMBrr72GtLQ0pKWlYebMmbXOrTZ79+7Fe++9h5UrV+Kzzz5D165dMW3aNHz22WcNnmNSUhJcXFzg6ekpja22y3RHjhzBmDFjUFhYiK1bt+Ljjz+GhYUFnnzyyWovm86cORNKpRI7d+5ETEwMDh8+jGeffbbB4yR6pAgiatN+++038fjjjwsAAoBQKpVi6NChIjo6Wty5c0equ3TpkjAwMBDPPPNMjX0VFBQIExMTMXHiRK32q1evCpVKJYKDg6W26dOnCwDio48+0qr9+OOPBQDx+eefa7WfPHlSABCbNm2qdT7h4eECgLh582a1+0tKSgQAMWHCBK2xdO3aVdp+5513BABx+/btGo9TNZ74+HidfTXNrbpjCSEEAGFiYiLy8vKktvLycuHu7i66d++uM7cHxcfHCwAiOztbauvVq5cYOXKkTm12drbOuL29vYWNjY3WepeXl4vevXsLR0dHUVlZqXWcOXPmaPUZExMjAIjc3Fyd4xG1FTxDRNTGWVtb49tvv8XJkyfx1ltvYdKkSbhw4QKWLVuGPn364LfffgMApKamoqKiAi+//HKNfaWlpaGkpAQzZszQandycsKYMWPw1Vdf6bzmL3/5i9b2/v370b59ezz55JMoLy+Xfvr37w87OzscPnz4oeYrhKizpupyWFBQED755BP897//bdSxHpxbbXx9fWFrayttGxgY4G9/+xt++eUX5OTkNOr49XH37l2kp6fj6aefhrm5udbxQ0JCkJOTg/Pnz2u9puosW5W+ffsCgNaZQqK2hoGISCYGDRqEpUuX4tNPP8X169fxyiuv4PLly9KN1VX37zg6OtbYx61btwD88STTgxwcHKT9VUxNTWFpaanVduPGDdy+fRtGRkZQKpVaP3l5eVJAa6yqP9oODg411owYMQL/+te/UF5ejueeew6Ojo7o3bs3Pv7443ofp7q51cbOzq7Gtgd/b02poKAAQoga16y641tbW2ttV92c/rCXM4laMz5lRiRDSqUS4eHhWLt2Lc6ePQsA6NSpEwAgJycHTk5O1b6u6g9lbm6uzr7r169rPfUFoNobhKtu1E1OTq72GBYWFvWfSDX27t0LABg1alStdZMmTcKkSZNQWlqK48ePIzo6GsHBwXjsscfg4+NT53Gqm1tt8vLyamyr+r0aGxsDAEpLS6UQAuChQmKHDh3Qrl27GtcMgM66EckRzxARtXHV/SEEID0VVnWWwN/fHwYGBoiNja2xLx8fH5iYmGDHjh1a7Tk5Ofj666+lJ61qExAQgFu3bqGiogKDBg3S+XFzc6vv1HT8+OOPiIqKwmOPPYagoKB6vUalUmHkyJFYvXo1AOD06dNSO9B0Z0W++uor3LhxQ9quqKjA7t270a1bN+msXNWTYj/99JPWa/ft21ftuOszNjMzM3h5eWHPnj1a9ZWVldixYwccHR3Ro0ePxkyJqE3hGSKiNm7cuHFwdHTEk08+CXd3d1RWViIjIwPvvvsuzM3NsWDBAgB//DH+xz/+gTfeeAMlJSWYNm0a1Go1MjMz8dtvvyEyMhLt27fH66+/jn/84x947rnnMG3aNNy6dQuRkZEwNjZGeHh4neOZOnUqEhMTMXHiRCxYsABDhgyBUqlETk4ODh06hEmTJuGpp56qs59Tp05BrVZDo9Hg+vXr+Oqrr7B9+3bY2Nhg3759MDIyqvG1K1asQE5ODnx9feHo6Ijbt29j/fr1UCqVGDlyJIA/PrvIxMQEiYmJ8PDwgLm5ORwcHGq9FFebjh07YsyYMXj99ddhZmaGTZs24eeff9Z69H7ixImwsrJCaGgoVq5cCUNDQyQkJODatWs6/fXp0we7du3C7t274eLiAmNjY/Tp06faY0dHR8PPzw+jR4/G4sWLYWRkhE2bNuHs2bP4+OOPG3y2i6hN0vdd3UTUvHbv3i2Cg4OFq6urMDc3F0qlUnTp0kWEhISIzMxMnfp//vOfYvDgwcLY2FiYm5sLT09PnSetPvzwQ9G3b19hZGQk1Gq1mDRpkjh37pxWzfTp04WZmVm1Y9JoNOKdd94R/fr1k47j7u4uZs+eLS5evFjrfKqexKr6UalUwt7eXvj7+4v169eLoqIindc8+OTX/v37xYQJE0Tnzp2FkZGRsLGxERMnThTffvut1us+/vhj4e7uLpRKpQAgwsPD65xbTU+Zvfzyy2LTpk2iW7duQqlUCnd3d5GYmKjz+hMnToihQ4cKMzMz0blzZxEeHi4+/PBDnafMLl++LPz9/YWFhYUAIB2zuqfMhBDi22+/FWPGjBFmZmbCxMREeHt7i3379mnVVD1ldvLkSa32Q4cOCQDi0KFD1c6ZqC1QCFGPRzKIiIiI2jDeQ0RERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHD2asp8rKSly/fh0WFhb8EDMiIqJHhBACd+7cgYODA9q1q/k8EANRPV2/fr3G73ciIiKi1u3atWu1fnk1A1E9VX3h5LVr1xr0Ddd10Wg0SElJgb+/P5RKZZP1Sw3DddA/rkHrwHVoHbgOTaeoqAhOTk51fnE0A1E9VV0ms7S0bPJAZGpqCktLS/6j1yOug/5xDVoHrkPrwHVoenXd7sKbqomIiEj2GIiIiIhI9hiIiIiISPZ4D1ETq6iogEajqXe9RqOBoaEh7t+/j4qKimYcGdWmseugVCphYGDQjCMjIqKWwEDURIQQyMvLw+3btxv8Ojs7O1y7do2fb6RHD7MO7du3h52dHdePiOgRxkDURKrCkI2NDUxNTev9x7GyshLFxcUwNzev9QOjqHk1Zh2EELh37x7y8/MBAPb29s05RCIiakYMRE2goqJCCkPW1tYNem1lZSXKyspgbGzMQKRHjV0HExMTAEB+fj5sbGx4+YyI6BHFv8BNoOqeIVNTUz2PhPShat0bcu8YERG1LgxETYj3kMgT152I6NHHQERERESyx0BEzS4iIgL9+/d/ZPolIiL54U3VzWjZnjN1FwmBMk0ZjJRGwENeeome0qdB9TNmzMC2bdsAAAYGBnBwcMATTzyBqKgodOjQ4aHG0hCXL1+Gs7NztfvS0tLg7e3dYmMhIiJ5YiCSufHjxyM+Ph7l5eXIzMzE3//+d9y+fRsff/xxi4/l4MGD6NWrl1ZbQ5/aIyIiagxeMpM5lUoFOzs7ODo6wt/fH3/729+QkpKiVRMfHw8PDw8YGxvD3d0dmzZt0tq/dOlS9OjRA6ampnBxccHrr7/eqCeurK2tYWdnp/Xz5295fuutt2BrawsLCwuEhobi/v37Wq8vLy/H/Pnz0b59e1hbW2Pp0qWYPn06Jk+eLNUIIRATEwMXFxeYmJigX79++Oyzz6T9BQUFeOaZZ9CpUyeYmJjA1dUV8fHxDZ4LERE9WvQeiP773//i2WefhbW1NUxNTdG/f3+cOnVK2i+EQEREBBwcHGBiYoJRo0bh3LlzWn2UlpZi3rx56NixI8zMzBAYGIicnBytmoKCAoSEhECtVkOtViMkJKTBnyrd1l26dAnJyclaISQuLg7Lly/Hm2++iaysLERFReH111+XLrUBgIWFBRISEpCZmYn169cjLi4Oa9eubdKxffLJJwgPD8ebb76J77//Hvb29jrBbPXq1UhMTER8fDy+++47FBUV4V//+pdWzWuvvYb4+HjExsbi3LlzeOWVV/Dss8/iyJEjAIAVK1YgMzMTX375JbKyshAbG4uOHTs26VyIiKj10esls4KCAgwbNgyjR4/Gl19+CRsbG/z6669o3769VBMTE4M1a9YgISEBPXr0wKpVq+Dn54fz58/DwsICABAWFoZ9+/Zh165dsLa2xqJFixAQEIBTp05JH5QXHByMnJwcJCcnAwBeeOEFhISEYN++fS0+79Zk//79MDc3R0VFhXTGZc2aNdL+N954A++++y6mTJkCAHB2dkZmZiY2b96M6dOnA/gjZFR57LHHsGjRIuzevRtLlixp0FiGDh2q86GIhYWFMDAwwLp16/D3v/8dM2fOBACsWrUKBw8e1DpLtGHDBixbtgxPPfUUAGDjxo04cOCAtP/u3btYs2YNvv76a/j4+AAAXFxccPToUWzZsgWxsbG4evUqPD09MWjQIGk+RET0cOpzT21D74NtanoNRKtXr4aTk5PWJYk//wESQmDdunVYvny59Ad527ZtsLW1xc6dOzF79mwUFhZi69at2L59O8aOHQsA2LFjB5ycnHDw4EGMGzcOWVlZSE5OxvHjx+Hl5QXgjzMfPj4+OH/+PNzc3Fpu0q3M6NGjERsbi3v37uHDDz/EhQsXMG/ePADAzZs3ce3aNYSGhmLWrFnSa8rLy6FWq6Xtzz77DOvWrcMvv/yC4uJilJeXw9LSssFj2b17Nzw8PLTaqgJtVlYWXnzxRa19Pj4+OHToEIA/gtONGzcwZMgQrdcOHDgQlZWVAIDMzEzcv38ffn5+Wv2UlZXB09MTAPDiiy/ir3/9K3744Qf4+/tj8uTJGDp0aIPnQkREjxa9XjLbu3cvBg0ahL/+9a+wsbGBp6cn4uLipP3Z2dnIy8uDv7+/1KZSqTBy5EgcO3YMAHDq1CloNBqtGgcHB/Tu3VuqSUtLg1qtlsIQAHh7e0OtVks1cmVmZobu3bujb9++eO+991BaWorIyEgAkIJEXFwcMjIypJ+zZ8/i+PHjAIDjx49j6tSpmDBhAvbv34/Tp09j+fLlKCsra/BYnJyc0L17d62fhnrwQxKFENL/rprPF198oTWfzMxMfPLJJwCACRMm4MqVKwgLC8P169fh6+uLxYsXN3gcRET0aNHrGaJLly4hNjYWCxcuxD/+8Q+cOHEC8+fPh0qlwnPPPYe8vDwAgK2trdbrbG1tceXKFQB/fKmqkZGRzmPitra20uvz8vJgY2Ojc3wbGxup5kGlpaUoLS2VtouKigD88fUMD94wrNFoIIRAZWWl9EcXAPCnP8Y1ERDS/1XUXV4rrWPXgxBCGneV119/HU888QRmz54NBwcHdO7cGb/++iumTZtW7fGOHj2Krl27YtmyZVL75cuXtcZTFUpqGl9Vu87v7088PDyQlpaGZ599VmqrCmWVlZWwsLCAra0t0tPTMWzYMAB/fMfc6dOn0a9fP1RWVsLd3R0qlQqXL1/G8OHDdX4Xd+7cgRAC1tbWeO655/Dcc89h2LBhWLp0KWJiYmr8PVZWVkIIAY1Gw+8yewhV7yt+BYp+cR1ah7a2Dgao++9Tc821vv3qNRBVVlZi0KBBiIqKAgB4enri3LlziI2NxXPPPSfVVff/9df1dQkP1lRXX1s/0dHR0pmSP0tJSdH5zjJDQ0PY2dmhuLhY68xImab+Z0ma4h9CVWhryDHLy8u1XjdgwAC4u7sjMjISb7/9NpYsWYJXX30VRkZGGDt2LEpLS5GRkYHbt2/j5ZdfhoODA65evYr4+HgMGDAAKSkpSEpKghBC6re0tBQVFRU1jq+4uBgAcO3aNZiZmWntU6vVMDY2xsyZMzFnzhz06tUL3t7e+PTTT3Hu3Dl07dpV6nfmzJmIjo6Gg4MDXF1dsWXLFvz+++9ax547dy4WLlyIe/fuwdvbG3fu3EF6ejrMzc0xbdo0LFu2DP3794e7uztKS0vx73//Gz169Kj1d1tWVoaSkhJ88803KC8vb9AakK7U1FR9D4HAdWgt2so6DK7H/6944MDlZjn2vXv36lWn10Bkb2+Pnj17arV5eHjg888/BwDY2dkB+OMMj729vVSTn58vnTWys7NDWVkZCgoKtM4S5efnS/d+2NnZ4caNGzrHv3nzps7ZpyrLli3DwoULpe2ioiI4OTnB399f5/6Y+/fv49q1azA3N4exsbHUbqQ0qvN3IPDHmQWlUgkFHu6DGRt6345SqYShoaHO6xYtWoTQ0FC89tprmDt3LqysrPDuu+8iPDwcZmZm6NOnD+bPnw9LS0tMnToVp0+fxtKlS1FaWoqJEyfi9ddfR2RkpNSvSqWCgYFBjeMzNzcHAK3H46skJiZi6tSpmDFjBnJzcxEZGYn79+9jypQpePHFF5GSkiL1u2LFCty+fRsvvfQSDAwMMGvWLIwbN07r2FX3ra1fvx4LFixA+/bt4enpiVdffVUay6pVq3D58mWYmJjg8ccfx+7du2v93d6/fx8mJiYYMWKE1vpTw2g0GqSmpsLPz0/rSUdqWVyH1qGtrUPkvsw6a8Kf7FlnTWPU92SBQoh6XNdpJsHBwbh27Rq+/fZbqe2VV15Beno6jh07BiEEHBwc8Morr0hPLJWVlcHGxgarV6+Wbqru1KkTduzYgaCgIABAbm4uHB0dceDAAemm6p49eyI9PV266TY9PR3e3t74+eef63VTdVFREdRqNQoLC6sNRNnZ2XB2dm7wH8TKykoUFRXB0tJS5wkrejiVlZXw8PBAUFAQ3njjjTprG7sOD7P+9D8ajQYHDhzAxIkT28QfgEcV16F1aGvroM+nzGr7+/1nej1D9Morr2Do0KGIiopCUFAQTpw4gS1btmDLli0A/rjMFRYWhqioKLi6usLV1RVRUVEwNTVFcHAwgD8uqYSGhmLRokWwtraGlZUVFi9ejD59+khPnXl4eGD8+PGYNWsWNm/eDOCPx+4DAgJk/YRZW3PlyhWkpKRg5MiRKC0txcaNG5GdnS39WyEiIqqJXgPR4MGDkZSUhGXLlmHlypVwdnbGunXr8Mwzz0g1S5YsQUlJCebMmYOCggJ4eXkhJSVF+gwiAFi7di0MDQ0RFBSEkpIS+Pr6IiEhQesG18TERMyfP196Gi0wMBAbN25suclSs2vXrh0SEhKwePFiCCHQu3dvHDx4UOdRfiIiogfp/bvMAgICEBAQUON+hUKBiIgIRERE1FhjbGyMDRs2YMOGDTXWWFlZYceOHQ8zVGrlnJyc8N133+l7GERE9AjiTStEREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DETUKAkJCWjfvr2+h4HLly9DoVAgIyND30MhIqJHmN4/h6hN27egzhKFEDAp00BhpATq+MLaOj25vkHlM2bMwLZt23TaL168iO7duz/cWBph1KhROHLkiE777Nmz8cEHH7T4eIiISD4YiGRu/PjxiI+P12rr1KmTnkYDzJo1CytXrtRqMzU11dNoiIhILnjJTOZUKhXs7Oy0fgwMDLBmzRr06dMHZmZmcHJywpw5c1BcXFxjP7du3cKQIUMQGBiI+/fvQwiBmJgYuLi4wMTEBP369cNnn31W53hMTU11xvPnL+M7ceIEPD09YWxsjEGDBuH06dM6fezduxeurq4wMTHB6NGjsW3bNigUCty+fVuqOXbsGEaMGAETExM4OTlhwYIFuHv3rrR/06ZNcHV1hbGxMWxtbfH000/X8zdKRESPIgYiqla7du3w3nvv4ezZs9i2bRu+/vprLFmypNranJwcDB8+HO7u7tizZw+MjY3x2muvIT4+HrGxsTh37hxeeeUVPPvss9VeEquvu3fvSl/Ie+rUKURERGDx4sVaNZcvX8bTTz+NyZMnIyMjA7Nnz8by5cu1as6cOYNx48ZhypQp+Omnn7B7925899130vy+//57zJ8/HytXrsT58+eRnJyMESNGNHrcRETU+vGSmczt378f5ubm0vaECRPw6aefIiwsTGpzdnbGG2+8gZdeegmbNm3Sev2FCxfg5+eHSZMmYf369VAoFLh79y7WrFmDr7/+Gj4+PgAAFxcXHD16FJs3b8bIkSNrHM+mTZvw4YcfarW9//77mD59OhITE1FRUYGPPvoIpqam6NWrF3JycvDSSy9JtR988AHc3Nzw9ttvAwDc3Nxw9uxZvPnmm1LN22+/jeDgYGmOrq6uWLduHUaPHo24uDhcvXoVZmZmCAgIgIWFBbp27QpPT8+G/WKJiOiRwkAkc6NHj0ZsbKy0bWZmBgA4dOgQoqKikJmZiaKiIpSXl+P+/fu4e/euVFNSUoLHH38c06ZNw/r1/7uhOzMzE/fv34efn5/WscrKyuoMFs8884zOGR0bGxsAQFZWFvr166d1T1FV4Kpy/vx5DB48WKttyJAhWtunTp3CL7/8gsTERKlNCIHKykpkZ2fDz88PXbt2hYuLC8aPH4/x48fjqaee4r1MRERtGAORzJmZmek8UXblyhVMnDgRL774It544w1YWVnh6NGjCA0NhUajkepUKhXGjh2LL774Av/3f/8HR0dHAEBlZSUA4IsvvkDnzp21+lapVLWOR61W1/iEmxCizvkIIaB44Gm9B19XWVmJ2bNnY/78+VptxcXF6NatG4yNjfHDDz/g8OHDSElJwYoVKxAREYGTJ0+2io8aICKipsd7iEjH999/j/Lycrz77rvw9vZGjx49cP36dZ26du3aYfv27Rg4cCDGjBkj1fTs2RMqlQpXr15F9+7dtX6cnJwaPa6ePXvixx9/RElJidR2/PhxrRp3d3ecPHlSZz5/NmDAAJw7d05nbC4uLjAyMgIAGBoaYuzYsYiJicFPP/2Ey5cv4+uvv2702ImIqHVjICId3bp1Q3l5OTZs2IBLly5h+/btNX4OkIGBARITE9GvXz+MGTMGeXl5sLCwwOLFi/HKK69g27Zt+PXXX3H69Gm8//771X7u0Z/du3cPeXl5Wj8FBQUAgODgYLRr1w6hoaHIzMzEgQMH8M4772i9fvbs2fj555+xdOlSXLhwAZ988gkSEhIAQDpztHTpUqSlpeHll19GRkYGLl68iL1790o3Ve/fvx/vvfceMjIycOXKFfzzn/9EZWUl3NzcHubXSkRErRgDEeno378/1qxZg9WrV6N3795ITExEdHR0jfWGhob4+OOP0atXL4wZMwb5+fl44403sGLFCkRHR8PDwwPjxo3Dvn374OzsXOux4+LiYG9vr/Uzbdo0AIC5uTn27duHzMxMeHp6Yvny5Vi9erXW652dnfHZZ59hz5496Nu3L2JjY6V7kqou1/Xt2xdHjhzBxYsXMXz4cHh6eiI8PBy2trYAgPbt22PPnj0YM2YMPDw88MEHH0jzIyKitkkh6nNjBqGoqAhqtRqFhYVan4sDAPfv30d2djacnZ1hbGzcoH4rKytRVFQES0tLtGvHfNoc3nzzTXzwwQe4du1ajTUPsw4Ps/70PxqNBgcOHMDEiROhVCr1PRzZ4jq0Dm1tHZbtOVNnTfSUPs1y7Nr+fv8Zb6qmNmfTpk0YPHgwrK2t8d133+Htt9/G3Llz9T0sIiJqxRiIqM25ePEiVq1ahd9//x1dunTBokWLsGzZMn0Pi4iIWjEGImpz1q5di7Vr1+p7GERE9AjhTStEREQkewxETYj3p8sT152I6NHHQNQEqp4AuHfvnp5HQvpQte5t4UkQIiK54j1ETcDAwADt27dHfn4+AMDU1FTn6yNqUllZibKyMty/f5+P3etRY9ZBCIF79+4hPz8f7du3h4GBQTOPkoiImgsDUROxs7MDACkU1ZcQAiUlJTAxMal3iKKm9zDr0L59e2n9iYjo0cRA1EQUCgXs7e1hY2Oj9QWoddFoNPjmm28wYsQIXnLRo8aug1Kp5JkhIqI2gIGoiRkYGDToD6SBgQHKy8thbGzMQKRHXAciInnjTStEREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQkewxEREREJHsMRERERCR7DEREREQke3oNRBEREVAoFFo/dnZ20n4hBCIiIuDg4AATExOMGjUK586d0+qjtLQU8+bNQ8eOHWFmZobAwEDk5ORo1RQUFCAkJARqtRpqtRohISG4fft2S0yRiIiIHgF6P0PUq1cv5ObmSj9nzpyR9sXExGDNmjXYuHEjTp48CTs7O/j5+eHOnTtSTVhYGJKSkrBr1y4cPXoUxcXFCAgIQEVFhVQTHByMjIwMJCcnIzk5GRkZGQgJCWnReRIREVHrZaj3ARgaap0VqiKEwLp167B8+XJMmTIFALBt2zbY2tpi586dmD17NgoLC7F161Zs374dY8eOBQDs2LEDTk5OOHjwIMaNG4esrCwkJyfj+PHj8PLyAgDExcXBx8cH58+fh5ubW8tNloiIiFolvZ8hunjxIhwcHODs7IypU6fi0qVLAIDs7Gzk5eXB399fqlWpVBg5ciSOHTsGADh16hQ0Go1WjYODA3r37i3VpKWlQa1WS2EIALy9vaFWq6UaIiIikje9niHy8vLCP//5T/To0QM3btzAqlWrMHToUJw7dw55eXkAAFtbW63X2Nra4sqVKwCAvLw8GBkZoUOHDjo1Va/Py8uDjY2NzrFtbGykmuqUlpaitLRU2i4qKgIAaDQaaDSaRsy2elV9NWWf1HBcB/3jGrQOXIfWoa2tgwEq66xprrnWt1+9BqIJEyZI/7tPnz7w8fFBt27dsG3bNnh7ewMAFAqF1muEEDptD3qwprr6uvqJjo5GZGSkTntKSgpMTU1rPX5jpKamNnmf1HBcB/3jGrQOXIfWoa2sw2CDumsOHLjcLMe+d+9ever0fg/Rn5mZmaFPnz64ePEiJk+eDOCPMzz29vZSTX5+vnTWyM7ODmVlZSgoKNA6S5Sfn4+hQ4dKNTdu3NA51s2bN3XOPv3ZsmXLsHDhQmm7qKgITk5O8Pf3h6Wl5UPN8880Gg1SU1Ph5+cHpVLZZP1Sw3Ad9I9r0DpwHVqHtrYOkfsy66wJf7Jnsxy76gpPXVpVICotLUVWVhaGDx8OZ2dn2NnZITU1FZ6engCAsrIyHDlyBKtXrwYADBw4EEqlEqmpqQgKCgIA5Obm4uzZs4iJiQEA+Pj4oLCwECdOnMCQIUMAAOnp6SgsLJRCU3VUKhVUKpVOu1KpbJZ/nM3VLzUM10H/uAatA9ehdWgr61BRj1uWm2ue9e1Xr4Fo8eLFePLJJ9GlSxfk5+dj1apVKCoqwvTp06FQKBAWFoaoqCi4urrC1dUVUVFRMDU1RXBwMABArVYjNDQUixYtgrW1NaysrLB48WL06dNHeurMw8MD48ePx6xZs7B582YAwAsvvICAgAA+YUZEREQA9ByIcnJyMG3aNPz222/o1KkTvL29cfz4cXTt2hUAsGTJEpSUlGDOnDkoKCiAl5cXUlJSYGFhIfWxdu1aGBoaIigoCCUlJfD19UVCQgIMDP53wTIxMRHz58+XnkYLDAzExo0bW3ayRERE1GrpNRDt2rWr1v0KhQIRERGIiIioscbY2BgbNmzAhg0baqyxsrLCjh07GjtMIiIiauP0/jlERERERPrGQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESy12oCUXR0NBQKBcLCwqQ2IQQiIiLg4OAAExMTjBo1CufOndN6XWlpKebNm4eOHTvCzMwMgYGByMnJ0aopKChASEgI1Go11Go1QkJCcPv27RaYFRERET0KWkUgOnnyJLZs2YK+fftqtcfExGDNmjXYuHEjTp48CTs7O/j5+eHOnTtSTVhYGJKSkrBr1y4cPXoUxcXFCAgIQEVFhVQTHByMjIwMJCcnIzk5GRkZGQgJCWmx+REREVHrpvdAVFxcjGeeeQZxcXHo0KGD1C6EwLp167B8+XJMmTIFvXv3xrZt23Dv3j3s3LkTAFBYWIitW7fi3XffxdixY+Hp6YkdO3bgzJkzOHjwIAAgKysLycnJ+PDDD+Hj4wMfHx/ExcVh//79OH/+vF7mTERERK2L3gPRyy+/jCeeeAJjx47Vas/OzkZeXh78/f2lNpVKhZEjR+LYsWMAgFOnTkGj0WjVODg4oHfv3lJNWloa1Go1vLy8pBpvb2+o1WqphoiIiOTNUJ8H37VrF3744QecPHlSZ19eXh4AwNbWVqvd1tYWV65ckWqMjIy0zixV1VS9Pi8vDzY2Njr929jYSDXVKS0tRWlpqbRdVFQEANBoNNBoNPWZXr1U9dWUfVLDcR30j2vQOnAdWoe2tg4GqKyzprnmWt9+9RaIrl27hgULFiAlJQXGxsY11ikUCq1tIYRO24MerKmuvq5+oqOjERkZqdOekpICU1PTWo/fGKmpqU3eJzUc10H/uAatA9ehdWgr6zDYoO6aAwcuN8ux7927V686vQWiU6dOIT8/HwMHDpTaKioq8M0332Djxo3S/T15eXmwt7eXavLz86WzRnZ2digrK0NBQYHWWaL8/HwMHTpUqrlx44bO8W/evKlz9unPli1bhoULF0rbRUVFcHJygr+/PywtLRs5a10ajQapqanw8/ODUqlssn6pYbgO+sc1aB24Dq1DW1uHyH2ZddaEP9mzWY5ddYWnLnoLRL6+vjhz5oxW2/PPPw93d3csXboULi4usLOzQ2pqKjw9PQEAZWVlOHLkCFavXg0AGDhwIJRKJVJTUxEUFAQAyM3NxdmzZxETEwMA8PHxQWFhIU6cOIEhQ4YAANLT01FYWCiFpuqoVCqoVCqddqVS2Sz/OJurX2oYroP+cQ1aB65D69BW1qGiHrcsN9c869uv3gKRhYUFevfurdVmZmYGa2trqT0sLAxRUVFwdXWFq6sroqKiYGpqiuDgYACAWq1GaGgoFi1aBGtra1hZWWHx4sXo06ePdJO2h4cHxo8fj1mzZmHz5s0AgBdeeAEBAQFwc3NrwRkTERFRa6XXm6rrsmTJEpSUlGDOnDkoKCiAl5cXUlJSYGFhIdWsXbsWhoaGCAoKQklJCXx9fZGQkAADg/9dsExMTMT8+fOlp9ECAwOxcePGFp8PERERtU6tKhAdPnxYa1uhUCAiIgIRERE1vsbY2BgbNmzAhg0baqyxsrLCjh07mmiURERE1Nbo/XOIiIiIiPSNgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkj4GIiIiIZI+BiIiIiGSPgYiIiIhkr1GByMXFBbdu3dJpv337NlxcXB56UEREREQtqVGB6PLly6ioqNBpLy0txX//+9+HHhQRERFRSzJsSPHevXul//2f//wHarVa2q6oqMBXX32Fxx57rMkGR0RERNQSGhSIJk+eDABQKBSYPn261j6lUonHHnsM7777bpMNjoiIiKglNCgQVVZWAgCcnZ1x8uRJdOzYsVkGRURERNSSGhSIqmRnZzf1OIiIiIj0plGBCAC++uorfPXVV8jPz5fOHFX56KOPHnpgRERERC2lUYEoMjISK1euxKBBg2Bvbw+FQtHU4yIiIiJqMY0KRB988AESEhIQEhLS1OMhIiIianGN+hyisrIyDB06tKnHQkRERKQXjQpEM2fOxM6dO5t6LERERER60ahLZvfv38eWLVtw8OBB9O3bF0qlUmv/mjVrmmRwRERERC2hUYHop59+Qv/+/QEAZ8+e1drHG6yJiIjoUdOoQHTo0KGmHgcRERGR3jTqHiIiIiKitqRRZ4hGjx5d66Wxr7/+utEDIiIiImppjQpEVfcPVdFoNMjIyMDZs2d1vvSViIiIqLVrVCBau3Ztte0REREoLi5+qAERERERtbQmvYfo2Wef5feYERER0SOnSQNRWloajI2Nm7JLIiIiombXqEtmU6ZM0doWQiA3Nxfff/89Xn/99SYZGBEREVFLaVQgUqvVWtvt2rWDm5sbVq5cCX9//yYZGBEREVFLaVQgio+Pb+pxEBEREelNowJRlVOnTiErKwsKhQI9e/aEp6dnU42LiIiIqMU06qbq/Px8jBkzBoMHD8b8+fMxd+5cDBw4EL6+vrh582a9+4mNjUXfvn1haWkJS0tL+Pj44Msvv5T2CyEQEREBBwcHmJiYYNSoUTh37pxWH6WlpZg3bx46duwIMzMzBAYGIicnR6umoKAAISEhUKvVUKvVCAkJwe3btxszdSIiImqDGhWI5s2bh6KiIpw7dw6///47CgoKcPbsWRQVFWH+/Pn17sfR0RFvvfUWvv/+e3z//fcYM2YMJk2aJIWemJgYrFmzBhs3bsTJkydhZ2cHPz8/3LlzR+ojLCwMSUlJ2LVrF44ePYri4mIEBASgoqJCqgkODkZGRgaSk5ORnJyMjIwMhISENGbqRERE1AY16pJZcnIyDh48CA8PD6mtZ8+eeP/99xt0U/WTTz6ptf3mm28iNjYWx48fR8+ePbFu3TosX75ceqpt27ZtsLW1xc6dOzF79mwUFhZi69at2L59O8aOHQsA2LFjB5ycnHDw4EGMGzcOWVlZSE5OxvHjx+Hl5QUAiIuLg4+PD86fPw83N7fG/AqIiIioDWnUGaLKykoolUqddqVSicrKykYNpKKiArt27cLdu3fh4+OD7Oxs5OXlaQUslUqFkSNH4tixYwD+uIdJo9Fo1Tg4OKB3795STVpaGtRqtRSGAMDb2xtqtVqqISIiInlr1BmiMWPGYMGCBfj444/h4OAAAPjvf/+LV155Bb6+vg3q68yZM/Dx8cH9+/dhbm6OpKQk9OzZUwortra2WvW2tra4cuUKACAvLw9GRkbo0KGDTk1eXp5UY2Njo3NcGxsbqaY6paWlKC0tlbaLiooA/PG9bRqNpkFzrE1VX03ZJzUc10H/uAatA9ehdWhr62CAuk+WNNdc69tvowLRxo0bMWnSJDz22GNwcnKCQqHA1atX0adPH+zYsaNBfbm5uSEjIwO3b9/G559/junTp+PIkSPSfoVCoVUvhNBpe9CDNdXV19VPdHQ0IiMjddpTUlJgampa6/EbIzU1tcn7pIbjOugf16B14Dq0Dm1lHQYb1F1z4MDlZjn2vXv36lXXqEDk5OSEH374Aampqfj5558hhEDPnj2l+3gawsjICN27dwcADBo0CCdPnsT69euxdOlSAH+c4bG3t5fq8/PzpbNGdnZ2KCsrQ0FBgdZZovz8fAwdOlSquXHjhs5xb968qXP26c+WLVuGhQsXSttFRUVwcnKCv78/LC0tGzzPmmg0GqSmpsLPz6/ay5DUMrgO+sc1aB24Dq1DW1uHyH2ZddaEP9mzWY5ddYWnLg0KRF9//TXmzp2L48ePw9LSEn5+fvDz8wMAFBYWolevXvjggw8wfPjwho/4/xNCoLS0FM7OzrCzs0Nqaqr0+UZlZWU4cuQIVq9eDQAYOHAglEolUlNTERQUBADIzc3F2bNnERMTAwDw8fFBYWEhTpw4gSFDhgAA0tPTUVhYKIWm6qhUKqhUKp12pVLZLP84m6tfahiug/5xDVoHrkPr0FbWoaIetyw31zzr22+DAtG6deswa9asas+QqNVqzJ49G2vWrKl3IPrHP/6BCRMmwMnJCXfu3MGuXbtw+PBhJCcnQ6FQICwsDFFRUXB1dYWrqyuioqJgamqK4OBg6ZihoaFYtGgRrK2tYWVlhcWLF6NPnz7S2SoPDw+MHz8es2bNwubNmwEAL7zwAgICAviEGREREQFoYCD68ccfpbMz1fH398c777xT7/5u3LiBkJAQ5ObmQq1Wo2/fvkhOTpbOOi1ZsgQlJSWYM2cOCgoK4OXlhZSUFFhYWEh9rF27FoaGhggKCkJJSQl8fX2RkJAAA4P/XbBMTEzE/PnzpafRAgMDsXHjxoZMnYiIiNqwBgWiGzdu1HrqydDQsEGfVL1169Za9ysUCkRERCAiIqLGGmNjY2zYsAEbNmyoscbKyqrBN3sTERGRfDToc4g6d+6MM2fO1Lj/p59+0roBmoiIiOhR0KBANHHiRKxYsQL379/X2VdSUoLw8HAEBAQ02eCIiIiIWkKDLpm99tpr2LNnD3r06IG5c+fCzc0NCoUCWVlZeP/991FRUYHly5c311iJiIiImkWDApGtrS2OHTuGl156CcuWLYMQAsAf9/qMGzcOmzZtqvWzfYiIiIhaowZ/MGPXrl1x4MABFBQU4JdffoEQAq6urjpfn0FERET0qGjUJ1UDQIcOHTB48OCmHAsRERGRXjTq2+6JiIiI2hIGIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj0GIiIiIpI9BiIiIiKSPQYiIiIikj1DfQ+A/r8vlwKKipr3P7m+5cZCREQkMzxDRERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREsqfXQBQdHY3BgwfDwsICNjY2mDx5Ms6fP69VI4RAREQEHBwcYGJiglGjRuHcuXNaNaWlpZg3bx46duwIMzMzBAYGIicnR6umoKAAISEhUKvVUKvVCAkJwe3bt5t7ikRERPQI0GsgOnLkCF5++WUcP34cqampKC8vh7+/P+7evSvVxMTEYM2aNdi4cSNOnjwJOzs7+Pn54c6dO1JNWFgYkpKSsGvXLhw9ehTFxcUICAhARUWFVBMcHIyMjAwkJycjOTkZGRkZCAkJadH5EhERUetkqM+DJycna23Hx8fDxsYGp06dwogRIyCEwLp167B8+XJMmTIFALBt2zbY2tpi586dmD17NgoLC7F161Zs374dY8eOBQDs2LEDTk5OOHjwIMaNG4esrCwkJyfj+PHj8PLyAgDExcXBx8cH58+fh5ubW8tOnIiIiFqVVnUPUWFhIQDAysoKAJCdnY28vDz4+/tLNSqVCiNHjsSxY8cAAKdOnYJGo9GqcXBwQO/evaWatLQ0qNVqKQwBgLe3N9RqtVRDRERE8qXXM0R/JoTAwoUL8fjjj6N3794AgLy8PACAra2tVq2trS2uXLki1RgZGaFDhw46NVWvz8vLg42Njc4xbWxspJoHlZaWorS0VNouKioCAGg0Gmg0msZMsVpVfWmEQV2FTXZM0iWtA3/PesM1aB24Dq1DW1sHA1TWWdNcc61vv60mEM2dOxc//fQTjh49qrNPoVBobQshdNoe9GBNdfW19RMdHY3IyEid9pSUFJiamtZ67MZIxeOAqKXgwIEmPybpSk1N1fcQZI9r0DpwHVqHtrIOg+v4//kB4MCBy81y7Hv37tWrrlUEonnz5mHv3r345ptv4OjoKLXb2dkB+OMMj729vdSen58vnTWys7NDWVkZCgoKtM4S5efnY+jQoVLNjRs3dI578+ZNnbNPVZYtW4aFCxdK20VFRXBycoK/vz8sLS0fYrbaNBoNUlNT4YejUCoqai6csLrJjkm6pHXw84NSqdT3cGSJa9A6cB1ah7a2DpH7MuusCX+yZ7Mcu+oKT130GoiEEJg3bx6SkpJw+PBhODs7a+13dnaGnZ0dUlNT4enpCQAoKyvDkSNHsHr1HwFh4MCBUCqVSE1NRVBQEAAgNzcXZ8+eRUxMDADAx8cHhYWFOHHiBIYMGQIASE9PR2FhoRSaHqRSqaBSqXTalUpls/zjVCoqag9EbeAN8ShorvWl+uMatA5ch9ahraxDRT1uWW6ueda3X70Gopdffhk7d+7Ev//9b1hYWEj386jVapiYmEChUCAsLAxRUVFwdXWFq6sroqKiYGpqiuDgYKk2NDQUixYtgrW1NaysrLB48WL06dNHeurMw8MD48ePx6xZs7B582YAwAsvvICAgAA+YUZERET6DUSxsbEAgFGjRmm1x8fHY8aMGQCAJUuWoKSkBHPmzEFBQQG8vLyQkpICCwsLqX7t2rUwNDREUFAQSkpK4Ovri4SEBBgY/O+iZWJiIubPny89jRYYGIiNGzc27wSJiIjokaD3S2Z1USgUiIiIQERERI01xsbG2LBhAzZs2FBjjZWVFXbs2NGYYRIREVEb16o+h4iIiIhIHxiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYM9T0AIiIiatsm58TUo2p7s4+jNjxDRERERLLHQERERESyx0BEREREssdARERERLLHQERERESyx0BEREREssdARERERLLHQERERESyp9dA9M033+DJJ5+Eg4MDFAoF/vWvf2ntF0IgIiICDg4OMDExwahRo3Du3DmtmtLSUsybNw8dO3aEmZkZAgMDkZOTo1VTUFCAkJAQqNVqqNVqhISE4Pbt2808OyIiInpU6DUQ3b17F/369cPGjRur3R8TE4M1a9Zg48aNOHnyJOzs7ODn54c7d+5INWFhYUhKSsKuXbtw9OhRFBcXIyAgABUVFVJNcHAwMjIykJycjOTkZGRkZCAkJKTZ50dERESPBr1+dceECRMwYcKEavcJIbBu3TosX74cU6ZMAQBs27YNtra22LlzJ2bPno3CwkJs3boV27dvx9ixYwEAO3bsgJOTEw4ePIhx48YhKysLycnJOH78OLy8vAAAcXFx8PHxwfnz5+Hm5tYykyUiIqJWq9XeQ5SdnY28vDz4+/tLbSqVCiNHjsSxY8cAAKdOnYJGo9GqcXBwQO/evaWatLQ0qNVqKQwBgLe3N9RqtVRDRERE8tZqv9w1Ly8PAGBra6vVbmtriytXrkg1RkZG6NChg05N1evz8vJgY2Oj07+NjY1UU53S0lKUlpZK20VFRQAAjUYDjUbTiBlVr6ovjTCoq7DJjkm6pHXg71lvuAatA9ehdWhr61CpqDtuNNdc69tvqw1EVRQKhda2EEKn7UEP1lRXX1c/0dHRiIyM1GlPSUmBqalpXcNusFQ8DohaCg4caPJjkq7U1FR9D0H2uAatA9ehdWgz69Blcp0lB5rp79y9e/fqVddqA5GdnR2AP87w2NvbS+35+fnSWSM7OzuUlZWhoKBA6yxRfn4+hg4dKtXcuHFDp/+bN2/qnH36s2XLlmHhwoXSdlFREZycnODv7w9LS8uHm9yfaDQapKamwg9HoVRU1Fw4YXWTHZN0Sevg5welUqnv4cgS16B14Dq0Dm1tHb7/4IU6awa9uKVZjl11hacurTYQOTs7w87ODqmpqfD09AQAlJWV4ciRI1i9+o9wMHDgQCiVSqSmpiIoKAgAkJubi7NnzyImJgYA4OPjg8LCQpw4cQJDhgwBAKSnp6OwsFAKTdVRqVRQqVQ67Uqlsln+cSoVFbUHojbwhngUNNf6Uv1xDVoHrkPr0FbWoZ0or7OmueZZ3371GoiKi4vxyy+/SNvZ2dnIyMiAlZUVunTpgrCwMERFRcHV1RWurq6IioqCqakpgoODAQBqtRqhoaFYtGgRrK2tYWVlhcWLF6NPnz7SU2ceHh4YP348Zs2ahc2bNwMAXnjhBQQEBPAJMyIiIgKg50D0/fffY/To0dJ21SWq6dOnIyEhAUuWLEFJSQnmzJmDgoICeHl5ISUlBRYWFtJr1q5dC0NDQwQFBaGkpAS+vr5ISEiAgcH/blJOTEzE/PnzpafRAgMDa/zsIyIiIpIfvQaiUaNGQYia7yRWKBSIiIhAREREjTXGxsbYsGEDNmzYUGONlZUVduzY8TBDJSIiojas1X4OEREREVFLYSAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2WMgIiIiItljICIiIiLZYyAiIiIi2TPU9wConvYtqLvmyfXNPw4iIqI2iGeIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPZ4U3VbwhuviYiIGoVniIiIiEj2GIiIiIhI9hiIiIiISPZ4DxERERE12rI9Z+qsmdz8w3hoDERywxuviYiIdPCSGREREckeAxERERHJHgMRERERyR7vISJdvM+IiIhkhoGIGoehiYiI2hAGImo+DE1ERPSIYCAiIiKiatXrM4ZyYlpgJM2PgYj0i2eRiIioFWAgIiIikqH6nP2REwaiVuL7KwVoJ8pr3O/lbNWCo2ll6nMWqb54tomIqN7ayuWw+pBVINq0aRPefvtt5ObmolevXli3bh2GDx+u72FRS6opXAkDACOBL5cCioqmORbDFxE1A57ZaR6yCUS7d+9GWFgYNm3ahGHDhmHz5s2YMGECMjMz0aVLF30Pj9qilrw/ivdiEVEDyensT33IJhCtWbMGoaGhmDlzJgBg3bp1+M9//oPY2FhER0freXQkW015ObA1Has+4evB8VR3lo4hjmSm6uyPASox2ACI3JeJige+VIJBpnnIIhCVlZXh1KlTePXVV7Xa/f39cezYMT2NiqgNa6rw1ZIhjv5Q0+XjxoRcPUvP/l3fQ2iwyf///1YqDJHfZTKe+O+6Wu8vpaYji0D022+/oaKiAra2tlrttra2yMvLq/Y1paWlKC0tlbYLCwsBAL///js0Gk2TjU2j0eDevXu4c78S7URljXUHs36rsy/PLu3rrDl99XaL9VMfLTnm2vqqVBjinuM9HMq5Va//+LTk70guqluD1vZ7fhTfGw1V43sh65kmPxbVrFJRWa+/DW3JrVu3mqXfO3fuAACEELXWySIQVVEoFFrbQgidtirR0dGIjIzUaXd2dm6WsVFr8E99D4C4Bq0E16F1kNk6LN3ZrN3fuXMHarW6xv2yCEQdO3aEgYGBztmg/Px8nbNGVZYtW4aFCxdK25WVlfj9999hbW1dY4hqjKKiIjg5OeHatWuwtLRssn6pYbgO+sc1aB24Dq0D16HpCCFw584dODg41Foni0BkZGSEgQMHIjU1FU899ZTUnpqaikmTJlX7GpVKBZVKpdXWvn37ZhujpaUl/9G3AlwH/eMatA5ch9aB69A0ajszVEUWgQgAFi5ciJCQEAwaNAg+Pj7YsmULrl69ihdffFHfQyMiIiI9k00g+tvf/oZbt25h5cqVyM3NRe/evXHgwAF07dpV30MjIiIiPZNNIAKAOXPmYM6cOfoehhaVSoXw8HCdy3PUsrgO+sc1aB24Dq0D16HlKURdz6ERERERtXHt6i4hIiIiatsYiIiIiEj2GIiIiIhI9hiIiIiISPYYiJrBpk2b4OzsDGNjYwwcOBDffvttjbWHDx+GQqHQ+fn555+16j7//HP07NkTKpUKPXv2RFJSUnNP45HW1GuQkJBQbc39+/dbYjqPrIasA/DHdwguX74cXbt2hUqlQrdu3fDRRx9p1fC90DBNvQZ8LzROQ9ZhxowZ1f6Oe/XqpVXH90ITE9Skdu3aJZRKpYiLixOZmZliwYIFwszMTFy5cqXa+kOHDgkA4vz58yI3N1f6KS8vl2qOHTsmDAwMRFRUlMjKyhJRUVHC0NBQHD9+vKWm9UhpjjWIj48XlpaWWvtzc3NbakqPpIaugxBCBAYGCi8vL5Gamiqys7NFenq6+O6776T9fC80THOsAd8LDdfQdbh9+7bW7/batWvCyspKhIeHSzV8LzQ9BqImNmTIEPHiiy9qtbm7u4tXX3212vqqP8YFBQU19hkUFCTGjx+v1TZu3DgxderUhx5vW9QcaxAfHy/UanUTjrLta+g6fPnll0KtVotbt27V2CffCw3THGvA90LDNXQdHpSUlCQUCoW4fPmy1Mb3QtPjJbMmVFZWhlOnTsHf31+r3d/fH8eOHav1tZ6enrC3t4evry8OHTqktS8tLU2nz3HjxtXZpxw11xoAQHFxMbp27QpHR0cEBATg9OnTTTr2tqQx67B3714MGjQIMTEx6Ny5M3r06IHFixejpKREquF7of6aaw0Avhca4mH+m1Rl69atGDt2rNY3K/C90PRk9UnVze23335DRUUFbG1ttdptbW2Rl5dX7Wvs7e2xZcsWDBw4EKWlpdi+fTt8fX1x+PBhjBgxAgCQl5fXoD7lrLnWwN3dHQkJCejTpw+Kioqwfv16DBs2DD/++CNcXV2bfV6Pmsasw6VLl3D06FEYGxsjKSkJv/32G+bMmYPff/9duoeF74X6a6414HuhYRqzDn+Wm5uLL7/8Ejt37tRq53uh6TEQNQOFQqG1LYTQaavi5uYGNzc3advHxwfXrl3DO++8I/0xbmif1PRr4O3tDW9vb6lm2LBhGDBgADZs2ID33nuvGWbQNjRkHSorK6FQKJCYmCh9M/WaNWvw9NNP4/3334eJiUmD+6SmXwO+Fxqnsf9uExIS0L59e0yePLnJ+qTq8ZJZE+rYsSMMDAx0Enp+fr5Okq+Nt7c3Ll68KG3b2dk9dJ9y0Vxr8KB27dph8ODBtdbIWWPWwd7eHp07d5b+EAOAh4cHhBDIyckBwPdCQzTXGjyI74XaPcx/k4QQ+OijjxASEgIjIyOtfXwvND0GoiZkZGSEgQMHIjU1Vas9NTUVQ4cOrXc/p0+fhr29vbTt4+Oj02dKSkqD+pSL5lqDBwkhkJGRUWuNnDVmHYYNG4br16+juLhYartw4QLatWsHR0dHAHwvNERzrcGD+F6o3cP8N+nIkSP45ZdfEBoaqrOP74VmoJdbuduwqscrt27dKjIzM0VYWJgwMzOTng549dVXRUhIiFS/du1akZSUJC5cuCDOnj0rXn31VQFAfP7551LNd999JwwMDMRbb70lsrKyxFtvvcXHK2vRHGsQEREhkpOTxa+//ipOnz4tnn/+eWFoaCjS09NbfH6Pioauw507d4Sjo6N4+umnxblz58SRI0eEq6urmDlzplTD90LDNMca8L3QcA1dhyrPPvus8PLyqrZPvheaHgNRM3j//fdF165dhZGRkRgwYIA4cuSItG/69Oli5MiR0vbq1atFt27dhLGxsejQoYN4/PHHxRdffKHT56effirc3NyEUqkU7u7uWn+sSVdTr0FYWJjo0qWLMDIyEp06dRL+/v7i2LFjLTWdR1ZD1kEIIbKyssTYsWOFiYmJcHR0FAsXLhT37t3TquF7oWGaeg34Xmichq7D7du3hYmJidiyZUuNffK90LQUQgih77NURERERPrEe4iIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiIiISPYYiIiIiEj2GIiIiIhI9hiIiKjVioiIQP/+/aXtGTNmVPsll83t8uXLUCgUyMjIaPFjE1HLYCAiogaZMWMGFAoFFAoFlEolXFxcsHjxYty9e7fZj71+/XokJCTUq7alQ8ylS5cwbdo0ODg4wNjYGI6Ojpg0aRIuXLjQIscnoodjqO8BENGjZ/z48YiPj4dGo8G3336LmTNn4u7du4iNjdWp1Wg0UCqVTXLcP38Le2tSVlYGPz8/uLu7Y8+ePbC3t0dOTg4OHDiAwsLCZjtuU/5uieSOZ4iIqMFUKhXs7Ozg5OSE4OBgPPPMM/jXv/4F4H+XuT766CO4uLhApVJBCIHCwkK88MILsLGxgaWlJcaMGYMff/xRq9+33noLtra2sLCwQGhoKO7fv6+1/8FLZpWVlVi9ejW6d+8OlUqFLl264M033wQAODs7AwA8PT2hUCgwatQo6XXx8fHw8PCAsbEx3N3dsWnTJq3jnDhxAp6enjA2NsagQYNw+vTpWn8fmZmZuHTpEjZt2gRvb2907doVw4YNw5tvvonBgwdLdTk5OZg6dSqsrKxgZmaGQYMGIT09XdofGxuLbt26wcjICG5ubti+fbvWcRQKBT744ANMmjQJZmZmWLVqFQBg3759GDhwIIyNjeHi4oLIyEiUl5fXOmYi0sYzRET00ExMTKDRaKTtX375BZ988gk+//xzGBgYAACeeOIJWFlZ4cCBA1Cr1di8eTN8fX1x4cIFWFlZ4ZNPPkF4eDjef/99DB8+HNu3b8d7770HFxeXGo+7bNkyxMXFYe3atXj88ceRm5uLn3/+GcAfoWbIkCE4ePAgevXqBSMjIwBAXFwcwsPDsXHjRnh6euL06dOYNWsWzMzMMH36dNy9excBAQEYM2YMduzYgezsbCxYsKDW+Xfq1Ant2rXDZ599hrCwMGnOf1ZcXIyRI0eic+fO2Lt3L+zs7PDDDz+gsrISAJCUlIQFCxZg3bp1GDt2LPbv34/nn38ejo6OGD16tNRPeHg4oqOjsXbtWhgYGOA///kPnn32Wbz33nsYPnw4fv31V7zwwgtSLRHVk56/XJaIHjHTp08XkyZNkrbT09OFtbW1CAoKEkIIER4eLpRKpcjPz5dqvvrqK2FpaSnu37+v1Ve3bt3E5s2bhRBC+Pj4iBdffFFrv5eXl+jXr1+1xy4qKhIqlUrExcVVO87s7GwBQJw+fVqr3cnJSezcuVOr7Y033hA+Pj5CCCE2b94srKysxN27d6X9sbGx1fb1Zxs3bhSmpqbCwsJCjB49WqxcuVL8+uuv0v7NmzcLCwsLcevWrWpfP3ToUDFr1iyttr/+9a9i4sSJ0jYAERYWplUzfPhwERUVpdW2fft2YW9vX+NYiUgXL5kRUYPt378f5ubmMDY2ho+PD0aMGIENGzZI+7t27YpOnTpJ26dOnUJxcTGsra1hbm4u/WRnZ+PXX38FAGRlZcHHx0frOA9u/1lWVhZKS0vh6+tb73HfvHkT165dQ2hoqNY4Vq1apTWOfv36wdTUtF7jqPLyyy8jLy8PO3bsgI+PDz799FP06tULqampAICMjAx4enrCysqqxvkMGzZMq23YsGHIysrSahs0aJDW9qlTp7By5Uqt+cyaNQu5ubm4d+9e3b8UIgLAS2ZE1AijR49GbGwslEolHBwcdG7sNTMz09qurKyEvb09Dh8+rNNX+/btGzUGExOTBr+m6vJUXFwcvLy8tPZVXeYSQjRqPABgYWGBwMBABAYGYtWqVRg3bhxWrVoFPz+/eo1XoVBobQshdNqq+91GRkZiypQpOv0ZGxs3YhZE8sQzRETUYGZmZujevTu6du1ar6ecBgwYgLy8PBgaGqJ79+5aPx07dgQAeHh44Pjx41qve3D7z1xdXWFiYoKvvvqq2v1V9wxVVFRIbba2tujcuTMuXbqkM46qm7B79uyJH3/8ESUlJfUaR00UCgXc3d2ljyPo27cvMjIy8Pvvv1db7+HhgaNHj2q1HTt2DB4eHrUeZ8CAATh//rzOfLp374527fifeKL64hkiImp2Y8eOhY+PDyZPnozVq1fDzc0N169fx4EDBzB58mQMGjQICxYswPTp0zFo0CA8/vjjSExMxLlz52q8qdrY2BhLly7FkiVLYGRkhGHDhuHmzZs4d+4cQkNDYWNjAxMTEyQnJ8PR0RHGxsZQq9WIiIjA/PnzYWlpiQkTJqC0tBTff/89CgoKsHDhQgQHB2P58uUIDQ3Fa6+9hsuXL+Odd96pdX4ZGRkIDw9HSEgIevbsCSMjIxw5cgQfffQRli5dCgCYNm0aoqKiMHnyZERHR8Pe3h6nT5+Gg4MDfHx88H//938ICgrCgAED4Ovri3379mHPnj04ePBgrcdesWIFAgIC4OTkhL/+9a9o164dfvrpJ5w5c0Z6Co2I6kHfNzER0aPlwZuqHxQeHq51I3SVoqIiMW/ePOHg4CCUSqVwcnISzzzzjLh69apU8+abb4qOHTsKc3NzMX36dLFkyZIab6oWQoiKigqxatUq0bVrV6FUKkWXLl20bjCOi4sTTk5Ool27dmLkyJFSe2Jioujfv78wMjISHTp0ECNGjBB79uyR9qelpYl+/foJIyMj0b9/f/H555/XelP1zZs3xfz580Xv3r2Fubm5sLCwEH369BHvvPOOqKiokOouX74s/vKXvwhLS0thamoqBg0aJNLT06X9mzZtEi4uLkKpVIoePXqIf/7zn1rHASCSkpJ0jp+cnCyGDh0qTExMhKWlpRgyZIjYsmVLtWMlouophHiIC+ZEREREbQAvMBMREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkewxEBEREZHsMRARERGR7DEQERERkez9PzlRTJn2akDeAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _ = gae_model.eval()\n",
    "    z = gae_model.encode(train_batch.x.squeeze(0), train_batch.edge_index.squeeze(0))\n",
    "    logits = gae_model.decode(z, train_batch.edge_label_index.squeeze(0)).view(-1)\n",
    "    probs = torch.sigmoid(logits).cpu()\n",
    "    \n",
    "    print(f\"Min score: {probs.min().item():.4f}, Max score: {probs.max().item():.4f}\")\n",
    "    print(f\"Fraction > 0.5: {(probs > 0.5).float().mean().item():.4f}\")\n",
    "    labels = train_batch.edge_label.squeeze(0).cpu()\n",
    "\n",
    "    real_scores = probs[labels == 1]\n",
    "    fake_scores = probs[labels == 0]\n",
    "\n",
    "    plt.hist(real_scores.numpy(), bins=50, alpha=0.6, label='Real Edges')\n",
    "    plt.hist(fake_scores.numpy(), bins=50, alpha=0.6, label='Fake Edges')\n",
    "    plt.xlabel(\"Predicted Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Score Distribution\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test Subgraph] Loss: 0.6582 | AUC: 0.8606 | F1: 0.6738 | Precision: 0.5080 | Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    _ = gae_model.eval()\n",
    "\n",
    "    test_batch = dataset[1]\n",
    "    z_test = gae_model.encode(test_batch.x.squeeze(0), test_batch.edge_index.squeeze(0))\n",
    "    edge_label_index_test = test_batch.edge_label_index.squeeze(0)\n",
    "    edge_label_test = test_batch.edge_label.squeeze(0).float()\n",
    "    logits_test = gae_model.decode(z_test, edge_label_index_test).view(-1)\n",
    "\n",
    "    loss_test = bce_loss(logits_test, edge_label_test)\n",
    "\n",
    "    probs_test = torch.sigmoid(logits_test).cpu()\n",
    "    preds_test = (probs_test > 0.5).float()\n",
    "    labels_test = edge_label_test.cpu()\n",
    "\n",
    "    auc = roc_auc_score(labels_test, probs_test)\n",
    "    f1 = f1_score(labels_test, preds_test)\n",
    "    precision = precision_score(labels_test, preds_test)\n",
    "    recall = recall_score(labels_test, preds_test)\n",
    "\n",
    "    print(f\"[Test Subgraph] Loss: {loss_test:.4f} | AUC: {auc:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(loader):\n",
    "    print(f\"\\nTraining on subgraph {batch_idx + 1} / {len(loader)}\")\n",
    "\n",
    "    for epoch in range(1, epochs_per_subgraph + 1):\n",
    "        _ = gae_model.train()\n",
    "        \n",
    "        # Step 1: Encode\n",
    "        z = gae_model.encode(batch.x.squeeze(0), batch.edge_index.squeeze(0))  # shape: [num_nodes, out_dim]\n",
    "\n",
    "        # Step 2: Decode links (positive + negative edges)\n",
    "        edge_label_index = batch.edge_label_index.squeeze(0)  # [2, N]\n",
    "        edge_label = batch.edge_label.squeeze(0).float()      # [N]\n",
    "\n",
    "        logits = gae_model.decode(z, edge_label_index).view(-1)\n",
    "\n",
    "        # Step 3: Compute BCE loss\n",
    "        loss = bce_loss(logits, edge_label)\n",
    "\n",
    "        # Step 4: Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Subgraph {batch_idx + 1} | Epoch {epoch:03d} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iii) Naive random splitting on entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "splitter = RandomLinkSplit(\n",
    "    is_undirected=False,\n",
    "    split_labels=True,          \n",
    "    add_negative_train_samples=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_val=0,\n",
    "    num_test=0,\n",
    ")\n",
    "train_data, val_data, test_data = splitter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_edge_label',\n",
       " 'test_mask',\n",
       " 'pos_edge_label_index',\n",
       " 'neg_edge_label_index',\n",
       " 'y',\n",
       " 'edge_index',\n",
       " 'train_mask',\n",
       " 'node_time',\n",
       " 'x',\n",
       " 'neg_edge_label']"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()\n",
    "train_data.pos_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# 2) Encoder + Classifier\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "    \n",
    "in_feats    = data.num_node_features\n",
    "hidden_dim  = 128\n",
    "out_dim     = 64\n",
    "num_classes = int(data.y[data.y != 2].max() + 1)\n",
    "\n",
    "encoder    = GAEEncoder(in_feats, hidden_dim, out_dim)\n",
    "gae_model  = GAE(encoder)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(gae_model.parameters()),\n",
    "    lr=0.001\n",
    ")\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "ce_loss  = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_gae_only():\n",
    "    gae_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # encode on train positive edges\n",
    "    z = gae_model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # AE loss (pos + neg)\n",
    "    pos = train_data.pos_edge_label_index\n",
    "    neg = train_data.neg_edge_label_index\n",
    "    edge_all = torch.cat([pos, neg], dim=1)\n",
    "    logits_ae = gae_model.decode(z, edge_all)\n",
    "    labels_ae = torch.cat([\n",
    "        torch.ones(pos.size(1), device=logits_ae.device),\n",
    "        torch.zeros(neg.size(1), device=logits_ae.device),\n",
    "    ])\n",
    "    loss_ae = bce_loss(logits_ae, labels_ae)\n",
    "    loss_ae.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_ae.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | AE Loss=0.7777\n",
      "Epoch 002 | AE Loss=0.7514\n",
      "Epoch 003 | AE Loss=0.7237\n",
      "Epoch 004 | AE Loss=0.7001\n",
      "Epoch 005 | AE Loss=0.6829\n",
      "Epoch 006 | AE Loss=0.6720\n",
      "Epoch 007 | AE Loss=0.6657\n",
      "Epoch 008 | AE Loss=0.6625\n",
      "Epoch 009 | AE Loss=0.6611\n",
      "Epoch 010 | AE Loss=0.6608\n",
      "Epoch 011 | AE Loss=0.6610\n",
      "Epoch 012 | AE Loss=0.6614\n",
      "Epoch 013 | AE Loss=0.6617\n",
      "Epoch 014 | AE Loss=0.6618\n",
      "Epoch 015 | AE Loss=0.6616\n",
      "Epoch 016 | AE Loss=0.6613\n",
      "Epoch 017 | AE Loss=0.6607\n",
      "Epoch 018 | AE Loss=0.6600\n",
      "Epoch 019 | AE Loss=0.6591\n",
      "Epoch 020 | AE Loss=0.6582\n",
      "Epoch 021 | AE Loss=0.6573\n",
      "Epoch 022 | AE Loss=0.6563\n",
      "Epoch 023 | AE Loss=0.6554\n",
      "Epoch 024 | AE Loss=0.6545\n",
      "Epoch 025 | AE Loss=0.6537\n",
      "Epoch 026 | AE Loss=0.6530\n",
      "Epoch 027 | AE Loss=0.6523\n",
      "Epoch 028 | AE Loss=0.6517\n",
      "Epoch 029 | AE Loss=0.6511\n",
      "Epoch 030 | AE Loss=0.6505\n",
      "Epoch 031 | AE Loss=0.6499\n",
      "Epoch 032 | AE Loss=0.6493\n",
      "Epoch 033 | AE Loss=0.6487\n",
      "Epoch 034 | AE Loss=0.6482\n",
      "Epoch 035 | AE Loss=0.6476\n",
      "Epoch 036 | AE Loss=0.6471\n",
      "Epoch 037 | AE Loss=0.6466\n",
      "Epoch 038 | AE Loss=0.6461\n",
      "Epoch 039 | AE Loss=0.6456\n",
      "Epoch 040 | AE Loss=0.6451\n",
      "Epoch 041 | AE Loss=0.6446\n",
      "Epoch 042 | AE Loss=0.6441\n",
      "Epoch 043 | AE Loss=0.6437\n",
      "Epoch 044 | AE Loss=0.6432\n",
      "Epoch 045 | AE Loss=0.6427\n",
      "Epoch 046 | AE Loss=0.6423\n",
      "Epoch 047 | AE Loss=0.6418\n",
      "Epoch 048 | AE Loss=0.6413\n",
      "Epoch 049 | AE Loss=0.6409\n",
      "Epoch 050 | AE Loss=0.6404\n",
      "Epoch 051 | AE Loss=0.6399\n",
      "Epoch 052 | AE Loss=0.6395\n",
      "Epoch 053 | AE Loss=0.6390\n",
      "Epoch 054 | AE Loss=0.6386\n",
      "Epoch 055 | AE Loss=0.6381\n",
      "Epoch 056 | AE Loss=0.6377\n",
      "Epoch 057 | AE Loss=0.6373\n",
      "Epoch 058 | AE Loss=0.6369\n",
      "Epoch 059 | AE Loss=0.6365\n",
      "Epoch 060 | AE Loss=0.6361\n",
      "Epoch 061 | AE Loss=0.6358\n",
      "Epoch 062 | AE Loss=0.6354\n",
      "Epoch 063 | AE Loss=0.6350\n",
      "Epoch 064 | AE Loss=0.6346\n",
      "Epoch 065 | AE Loss=0.6343\n",
      "Epoch 066 | AE Loss=0.6339\n",
      "Epoch 067 | AE Loss=0.6335\n",
      "Epoch 068 | AE Loss=0.6331\n",
      "Epoch 069 | AE Loss=0.6328\n",
      "Epoch 070 | AE Loss=0.6324\n",
      "Epoch 071 | AE Loss=0.6321\n",
      "Epoch 072 | AE Loss=0.6317\n",
      "Epoch 073 | AE Loss=0.6313\n",
      "Epoch 074 | AE Loss=0.6310\n",
      "Epoch 075 | AE Loss=0.6307\n",
      "Epoch 076 | AE Loss=0.6303\n",
      "Epoch 077 | AE Loss=0.6300\n",
      "Epoch 078 | AE Loss=0.6297\n",
      "Epoch 079 | AE Loss=0.6294\n",
      "Epoch 080 | AE Loss=0.6290\n",
      "Epoch 081 | AE Loss=0.6287\n",
      "Epoch 082 | AE Loss=0.6284\n",
      "Epoch 083 | AE Loss=0.6281\n",
      "Epoch 084 | AE Loss=0.6278\n",
      "Epoch 085 | AE Loss=0.6275\n",
      "Epoch 086 | AE Loss=0.6272\n",
      "Epoch 087 | AE Loss=0.6270\n",
      "Epoch 088 | AE Loss=0.6267\n",
      "Epoch 089 | AE Loss=0.6264\n",
      "Epoch 090 | AE Loss=0.6261\n",
      "Epoch 091 | AE Loss=0.6259\n",
      "Epoch 092 | AE Loss=0.6256\n",
      "Epoch 093 | AE Loss=0.6254\n",
      "Epoch 094 | AE Loss=0.6251\n",
      "Epoch 095 | AE Loss=0.6248\n",
      "Epoch 096 | AE Loss=0.6246\n",
      "Epoch 097 | AE Loss=0.6243\n",
      "Epoch 098 | AE Loss=0.6241\n",
      "Epoch 099 | AE Loss=0.6239\n",
      "Epoch 100 | AE Loss=0.6236\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss_ae = train_gae_only()\n",
    "    print(f\"Epoch {epoch:03d} | AE Loss={loss_ae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss=2.0568 (AE=0.8042, CLF=1.2525)\n",
      "Epoch 002 | Loss=1.1343 (AE=0.8119, CLF=0.3224)\n",
      "Epoch 003 | Loss=1.2103 (AE=0.8132, CLF=0.3970)\n",
      "Epoch 004 | Loss=1.2418 (AE=0.8133, CLF=0.4285)\n",
      "Epoch 005 | Loss=1.2213 (AE=0.8133, CLF=0.4080)\n",
      "Epoch 006 | Loss=1.1763 (AE=0.8133, CLF=0.3631)\n",
      "Epoch 007 | Loss=1.1335 (AE=0.8133, CLF=0.3203)\n",
      "Epoch 008 | Loss=1.1224 (AE=0.8133, CLF=0.3091)\n",
      "Epoch 009 | Loss=1.1649 (AE=0.8133, CLF=0.3516)\n",
      "Epoch 010 | Loss=1.1584 (AE=0.8133, CLF=0.3452)\n",
      "Test Acc=0.9343 | Precision=0.7440 | Recall=0.5209 | F1=0.5240\n",
      "Epoch 011 | Loss=1.1139 (AE=0.8133, CLF=0.3006)\n",
      "Epoch 012 | Loss=1.1050 (AE=0.8133, CLF=0.2917)\n",
      "Epoch 013 | Loss=1.1207 (AE=0.8133, CLF=0.3075)\n",
      "Epoch 014 | Loss=1.1327 (AE=0.8133, CLF=0.3194)\n",
      "Epoch 015 | Loss=1.1276 (AE=0.8133, CLF=0.3144)\n",
      "Epoch 016 | Loss=1.1092 (AE=0.8133, CLF=0.2959)\n",
      "Epoch 017 | Loss=1.0934 (AE=0.8133, CLF=0.2801)\n",
      "Epoch 018 | Loss=1.0981 (AE=0.8133, CLF=0.2848)\n",
      "Epoch 019 | Loss=1.1078 (AE=0.8133, CLF=0.2945)\n",
      "Epoch 020 | Loss=1.0938 (AE=0.8133, CLF=0.2805)\n",
      "Test Acc=0.9261 | Precision=0.6757 | Recall=0.6083 | F1=0.6318\n",
      "Epoch 021 | Loss=1.0855 (AE=0.8133, CLF=0.2722)\n",
      "Epoch 022 | Loss=1.0884 (AE=0.8133, CLF=0.2752)\n",
      "Epoch 023 | Loss=1.0926 (AE=0.8133, CLF=0.2793)\n",
      "Epoch 024 | Loss=1.0907 (AE=0.8133, CLF=0.2774)\n",
      "Epoch 025 | Loss=1.0831 (AE=0.8133, CLF=0.2699)\n",
      "Epoch 026 | Loss=1.0765 (AE=0.8133, CLF=0.2632)\n",
      "Epoch 027 | Loss=1.0788 (AE=0.8133, CLF=0.2655)\n",
      "Epoch 028 | Loss=1.0808 (AE=0.8133, CLF=0.2675)\n",
      "Epoch 029 | Loss=1.0718 (AE=0.8133, CLF=0.2586)\n",
      "Epoch 030 | Loss=1.0700 (AE=0.8133, CLF=0.2567)\n",
      "Test Acc=0.9336 | Precision=0.7138 | Recall=0.5231 | F1=0.5280\n",
      "Epoch 031 | Loss=1.0718 (AE=0.8133, CLF=0.2585)\n",
      "Epoch 032 | Loss=1.0703 (AE=0.8133, CLF=0.2571)\n",
      "Epoch 033 | Loss=1.0662 (AE=0.8133, CLF=0.2529)\n",
      "Epoch 034 | Loss=1.0646 (AE=0.8133, CLF=0.2513)\n",
      "Epoch 035 | Loss=1.0657 (AE=0.8133, CLF=0.2525)\n",
      "Epoch 036 | Loss=1.0632 (AE=0.8133, CLF=0.2499)\n",
      "Epoch 037 | Loss=1.0593 (AE=0.8133, CLF=0.2460)\n",
      "Epoch 038 | Loss=1.0593 (AE=0.8133, CLF=0.2460)\n",
      "Epoch 039 | Loss=1.0596 (AE=0.8133, CLF=0.2463)\n",
      "Epoch 040 | Loss=1.0575 (AE=0.8133, CLF=0.2442)\n",
      "Test Acc=0.9155 | Precision=0.5861 | Recall=0.5460 | F1=0.5569\n",
      "Epoch 041 | Loss=1.0562 (AE=0.8133, CLF=0.2430)\n",
      "Epoch 042 | Loss=1.0560 (AE=0.8133, CLF=0.2427)\n",
      "Epoch 043 | Loss=1.0532 (AE=0.8133, CLF=0.2399)\n",
      "Epoch 044 | Loss=1.0518 (AE=0.8133, CLF=0.2386)\n",
      "Epoch 045 | Loss=1.0517 (AE=0.8133, CLF=0.2385)\n",
      "Epoch 046 | Loss=1.0509 (AE=0.8133, CLF=0.2376)\n",
      "Epoch 047 | Loss=1.0497 (AE=0.8133, CLF=0.2364)\n",
      "Epoch 048 | Loss=1.0487 (AE=0.8133, CLF=0.2355)\n",
      "Epoch 049 | Loss=1.0473 (AE=0.8133, CLF=0.2340)\n",
      "Epoch 050 | Loss=1.0463 (AE=0.8133, CLF=0.2330)\n",
      "Test Acc=0.9192 | Precision=0.5852 | Recall=0.5364 | F1=0.5463\n",
      "Epoch 051 | Loss=1.0459 (AE=0.8133, CLF=0.2326)\n",
      "Epoch 052 | Loss=1.0447 (AE=0.8133, CLF=0.2315)\n",
      "Epoch 053 | Loss=1.0434 (AE=0.8133, CLF=0.2301)\n",
      "Epoch 054 | Loss=1.0423 (AE=0.8133, CLF=0.2291)\n",
      "Epoch 055 | Loss=1.0413 (AE=0.8133, CLF=0.2281)\n",
      "Epoch 056 | Loss=1.0405 (AE=0.8133, CLF=0.2273)\n",
      "Epoch 057 | Loss=1.0396 (AE=0.8133, CLF=0.2264)\n",
      "Epoch 058 | Loss=1.0384 (AE=0.8133, CLF=0.2252)\n",
      "Epoch 059 | Loss=1.0375 (AE=0.8133, CLF=0.2243)\n",
      "Epoch 060 | Loss=1.0366 (AE=0.8133, CLF=0.2234)\n",
      "Test Acc=0.9171 | Precision=0.5931 | Recall=0.5477 | F1=0.5597\n",
      "Epoch 061 | Loss=1.0355 (AE=0.8133, CLF=0.2223)\n",
      "Epoch 062 | Loss=1.0346 (AE=0.8133, CLF=0.2213)\n",
      "Epoch 063 | Loss=1.0337 (AE=0.8133, CLF=0.2204)\n",
      "Epoch 064 | Loss=1.0328 (AE=0.8133, CLF=0.2195)\n",
      "Epoch 065 | Loss=1.0316 (AE=0.8133, CLF=0.2184)\n",
      "Epoch 066 | Loss=1.0306 (AE=0.8133, CLF=0.2174)\n",
      "Epoch 067 | Loss=1.0297 (AE=0.8133, CLF=0.2164)\n",
      "Epoch 068 | Loss=1.0287 (AE=0.8133, CLF=0.2154)\n",
      "Epoch 069 | Loss=1.0276 (AE=0.8133, CLF=0.2143)\n",
      "Epoch 070 | Loss=1.0267 (AE=0.8133, CLF=0.2134)\n",
      "Test Acc=0.9155 | Precision=0.5949 | Recall=0.5537 | F1=0.5660\n",
      "Epoch 071 | Loss=1.0256 (AE=0.8133, CLF=0.2124)\n",
      "Epoch 072 | Loss=1.0246 (AE=0.8133, CLF=0.2113)\n",
      "Epoch 073 | Loss=1.0236 (AE=0.8133, CLF=0.2104)\n",
      "Epoch 074 | Loss=1.0226 (AE=0.8133, CLF=0.2093)\n",
      "Epoch 075 | Loss=1.0215 (AE=0.8133, CLF=0.2082)\n",
      "Epoch 076 | Loss=1.0205 (AE=0.8133, CLF=0.2073)\n",
      "Epoch 077 | Loss=1.0194 (AE=0.8133, CLF=0.2062)\n",
      "Epoch 078 | Loss=1.0184 (AE=0.8133, CLF=0.2052)\n",
      "Epoch 079 | Loss=1.0173 (AE=0.8133, CLF=0.2041)\n",
      "Epoch 080 | Loss=1.0163 (AE=0.8133, CLF=0.2030)\n",
      "Test Acc=0.9134 | Precision=0.5954 | Recall=0.5599 | F1=0.5717\n",
      "Epoch 081 | Loss=1.0153 (AE=0.8133, CLF=0.2020)\n",
      "Epoch 082 | Loss=1.0142 (AE=0.8133, CLF=0.2009)\n",
      "Epoch 083 | Loss=1.0131 (AE=0.8133, CLF=0.1999)\n",
      "Epoch 084 | Loss=1.0120 (AE=0.8133, CLF=0.1988)\n",
      "Epoch 085 | Loss=1.0110 (AE=0.8133, CLF=0.1977)\n",
      "Epoch 086 | Loss=1.0099 (AE=0.8133, CLF=0.1966)\n",
      "Epoch 087 | Loss=1.0088 (AE=0.8133, CLF=0.1956)\n",
      "Epoch 088 | Loss=1.0078 (AE=0.8133, CLF=0.1945)\n",
      "Epoch 089 | Loss=1.0067 (AE=0.8133, CLF=0.1934)\n",
      "Epoch 090 | Loss=1.0057 (AE=0.8133, CLF=0.1924)\n",
      "Test Acc=0.9128 | Precision=0.5935 | Recall=0.5596 | F1=0.5710\n",
      "Epoch 091 | Loss=1.0046 (AE=0.8133, CLF=0.1913)\n",
      "Epoch 092 | Loss=1.0036 (AE=0.8133, CLF=0.1903)\n",
      "Epoch 093 | Loss=1.0025 (AE=0.8133, CLF=0.1893)\n",
      "Epoch 094 | Loss=1.0015 (AE=0.8133, CLF=0.1883)\n",
      "Epoch 095 | Loss=1.0006 (AE=0.8133, CLF=0.1874)\n",
      "Epoch 096 | Loss=0.9997 (AE=0.8133, CLF=0.1864)\n",
      "Epoch 097 | Loss=0.9990 (AE=0.8133, CLF=0.1858)\n",
      "Epoch 098 | Loss=0.9985 (AE=0.8133, CLF=0.1852)\n",
      "Epoch 099 | Loss=0.9996 (AE=0.8133, CLF=0.1863)\n",
      "Epoch 100 | Loss=0.9979 (AE=0.8133, CLF=0.1846)\n",
      "Test Acc=0.9166 | Precision=0.5954 | Recall=0.5513 | F1=0.5637\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# 2) Encoder + Classifier\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels, num_classes)\n",
    "    def forward(self, z):\n",
    "        return self.lin(z)\n",
    "\n",
    "in_feats    = data.num_node_features\n",
    "hidden_dim  = 128\n",
    "out_dim     = 64\n",
    "num_classes = int(data.y[data.y != 2].max() + 1)\n",
    "\n",
    "encoder    = GAEEncoder(in_feats, hidden_dim, out_dim)\n",
    "gae_model  = GAE(encoder)\n",
    "classifier = NodeClassifier(out_dim, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(gae_model.parameters()) + list(classifier.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "ce_loss  = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 3) train() 改用 train_pos/neg\n",
    "def train():\n",
    "    gae_model.train()\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # encode on train positive edges\n",
    "    z = gae_model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # AE loss (pos + neg)\n",
    "    pos = train_data.pos_edge_label_index\n",
    "    neg = train_data.neg_edge_label_index\n",
    "    edge_all = torch.cat([pos, neg], dim=1)\n",
    "    logits_ae = gae_model.decode(z, edge_all)\n",
    "    labels_ae = torch.cat([\n",
    "        torch.ones(pos.size(1), device=logits_ae.device),\n",
    "        torch.zeros(neg.size(1), device=logits_ae.device),\n",
    "    ])\n",
    "    loss_ae = bce_loss(logits_ae, labels_ae)\n",
    "\n",
    "    # classification loss (only known & train_mask)\n",
    "    # print(len(train_data.y != 2))\n",
    "    train_mask = torch.tensor(train_data.train_mask.values, dtype=torch.bool)\n",
    "   \n",
    "    for i in range(len(train_mask)):\n",
    "        train_mask[i] = train_mask[i] and (train_data.y[i] != 2)\n",
    "    mask = train_mask\n",
    "    #print(mask[mask == True].shape)\n",
    "    #print(mask.shape)\n",
    "    logits_cls = classifier(z)\n",
    "    loss_cls = ce_loss(logits_cls[mask], train_data.y[mask])\n",
    "    #print(loss_ae)\n",
    "    loss = loss_ae + loss_cls\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), loss_ae.item(), loss_cls.item()\n",
    "\n",
    "# 4) evaluate() 只用 train_data.train_pos_edge_index encode\n",
    "def evaluate():\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        z = gae_model.encode(data.x, data.edge_index) \n",
    "        logits = classifier(z)\n",
    "        preds  = logits.argmax(dim=1)\n",
    "   \n",
    "        test_mask = torch.tensor(data.test_mask.values, dtype=torch.bool) & (data.y != 2)\n",
    "  \n",
    "        mask = test_mask\n",
    "        y_true = data.y[mask].cpu().numpy()\n",
    "        y_pred = preds[mask].cpu().numpy()\n",
    "\n",
    "        acc  = (y_true == y_pred).mean().item()\n",
    "        prec = precision_score(y_true, y_pred, average='macro')\n",
    "        rec  = recall_score(y_true, y_pred, average='macro')\n",
    "        f1   = f1_score(y_true, y_pred, average='macro')\n",
    "        print(f\"Test Acc={acc:.4f} | \"\n",
    "              f\"Precision={prec:.4f} | \"\n",
    "              f\"Recall={rec:.4f} | \"\n",
    "              f\"F1={f1:.4f}\")\n",
    "        return acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss_tot, loss_ae, loss_cls = train()\n",
    "    print(f\"Epoch {epoch:03d} | \"\n",
    "              f\"Loss={loss_tot:.4f} (AE={loss_ae:.4f}, CLF={loss_cls:.4f})\")\n",
    "    if epoch % 10 == 0:\n",
    "        test_acc= evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elliptic-fork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
